{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b4198",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from scipy.signal import convolve2d\n",
    "import wandb\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from lux.utils import direction_to\n",
    "\n",
    "SPACE_SIZE   = 24\n",
    "FRAME_FACTOR = None  # will be set in Space.__init__\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity    = capacity\n",
    "        self.buffer      = deque(maxlen=capacity)\n",
    "        self.priorities  = deque(maxlen=capacity)\n",
    "        self.alpha       = alpha\n",
    "        self.beta_start  = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame       = 1\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_p = max(self.priorities) if self.priorities else 1.0\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(float(max_p))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        beta = min(\n",
    "            1.0,\n",
    "            self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames\n",
    "        )\n",
    "        self.frame += 1\n",
    "\n",
    "        prio_arr = np.array(self.priorities, dtype=np.float32)\n",
    "        probs    = prio_arr ** self.alpha\n",
    "        probs   /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        device   = samples[0][0].device\n",
    "        weights  = torch.as_tensor(weights, dtype=torch.float32, device=device)\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, err in zip(indices, errors):\n",
    "            p_val = (abs(err) + 1e-5) ** self.alpha\n",
    "            self.priorities[idx] = float(p_val)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels, action_size, local_view_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, input_channels, local_view_size, local_view_size)\n",
    "            x = self.pool(F.relu(self.conv1(dummy)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            conv_output_size = int(x.numel())\n",
    "\n",
    "        self.fc_shared = nn.Linear(conv_output_size, hidden_size)\n",
    "\n",
    "        self.value_fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.value_fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        self.adv_fc1   = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.adv_fc2   = nn.Linear(hidden_size // 2, action_size)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(obs))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc_shared(x))\n",
    "\n",
    "        v = F.relu(self.value_fc1(x))\n",
    "        v = self.value_fc2(v)\n",
    "\n",
    "        a = F.relu(self.adv_fc1(x))\n",
    "        a = self.adv_fc2(a)\n",
    "\n",
    "        a_mean = a.mean(dim=1, keepdim=True)\n",
    "        q      = v + (a - a_mean)\n",
    "        return q\n",
    "\n",
    "\n",
    "class Global:\n",
    "    MAX_UNITS                           = 16\n",
    "    RELIC_REWARD_RANGE                 = 5\n",
    "    ALL_RELICS_FOUND                   = False\n",
    "    ALL_REWARDS_FOUND                  = False\n",
    "    LAST_MATCH_WHEN_RELIC_CAN_APPEAR   = 10\n",
    "    LAST_MATCH_STEP_WHEN_RELIC_CAN_APPEAR = 0\n",
    "    REWARD_RESULTS                     = []\n",
    "\n",
    "\n",
    "class NodeType(IntEnum):\n",
    "    unknown  = -1\n",
    "    empty    = 0\n",
    "    nebula   = 1\n",
    "    asteroid = 2\n",
    "\n",
    "\n",
    "_xs, _ys = np.meshgrid(np.arange(SPACE_SIZE),\n",
    "                       np.arange(SPACE_SIZE),\n",
    "                       indexing=\"ij\")\n",
    "OPP_X = SPACE_SIZE - 1 - _xs\n",
    "OPP_Y = SPACE_SIZE - 1 - _ys\n",
    "\n",
    "NEIGHBOR_OFFSETS = {\n",
    "    r: [(dx,dy) for dx in range(-r, r+1) for dy in range(-r, r+1)]\n",
    "    for r in range(Global.RELIC_REWARD_RANGE + 1)\n",
    "}\n",
    "\n",
    "\n",
    "def warp_point(x, y):         return (x % SPACE_SIZE, y % SPACE_SIZE)\n",
    "def get_opposite(x, y):       return (SPACE_SIZE - 1 - x, SPACE_SIZE - 1 - y)\n",
    "def manhattan_distance(a, b): return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "\n",
    "ACTION_DIRS = [(0,0),(0,-1),(1,0),(0,1),(-1,0),(0,0)]\n",
    "class ActionType(IntEnum):\n",
    "    center = 0; up = 1; right = 2; down = 3; left = 4; sap = 5\n",
    "    def to_direction(self): return ACTION_DIRS[self.value]\n",
    "    @classmethod\n",
    "    def from_coordinates(cls, cur, nxt):\n",
    "        dx, dy = nxt[0]-cur[0], nxt[1]-cur[1]\n",
    "        for v, (mx, my) in enumerate(ACTION_DIRS[:5]):\n",
    "            if (dx,dy) == (mx,my):\n",
    "                return cls(v)\n",
    "        return cls.center\n",
    "\n",
    "MIRROR_ACTION = {\n",
    "    ActionType.center.value: ActionType.center.value,\n",
    "    ActionType.up.value:     ActionType.down.value,\n",
    "    ActionType.down.value:   ActionType.up.value,\n",
    "    ActionType.left.value:   ActionType.right.value,\n",
    "    ActionType.right.value:  ActionType.left.value,\n",
    "}\n",
    "\n",
    "\n",
    "class Node:\n",
    "    __slots__ = (\"x\",\"y\",\"type\",\"energy\",\"is_visible\",\n",
    "                 \"_relic\",\"_reward\",\"_explored_for_relic\",\"_explored_for_reward\")\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "        self.type      = NodeType.unknown\n",
    "        self.energy    = None\n",
    "        self.is_visible= False\n",
    "        self._relic               = False\n",
    "        self._reward              = False\n",
    "        self._explored_for_relic  = False\n",
    "        self._explored_for_reward = False\n",
    "\n",
    "    @property\n",
    "    def coordinates(self): return (self.x, self.y)\n",
    "    @property\n",
    "    def relic(self):       return self._relic\n",
    "    @property\n",
    "    def reward(self):      return self._reward\n",
    "    @property\n",
    "    def explored_for_relic(self):  return self._explored_for_relic\n",
    "    @property\n",
    "    def explored_for_reward(self): return self._explored_for_reward\n",
    "\n",
    "    def update_relic_status(self, status):\n",
    "        if status is None:\n",
    "            self._explored_for_relic = False\n",
    "            return\n",
    "        if self._explored_for_relic and self._relic and not status:\n",
    "            raise ValueError(f\"Cannot flip relic at {self.coordinates}\")\n",
    "        self._relic              = status\n",
    "        self._explored_for_relic = True\n",
    "\n",
    "    def update_reward_status(self, status):\n",
    "        if status is None:\n",
    "            self._explored_for_reward = False\n",
    "            return\n",
    "        if self._explored_for_reward and self._reward and not status:\n",
    "            return\n",
    "        self._reward               = status\n",
    "        self._explored_for_reward  = True\n",
    "\n",
    "\n",
    "class Ship:\n",
    "    def __init__(self, uid):\n",
    "        self.unit_id = uid\n",
    "        self.clean()\n",
    "    def clean(self):\n",
    "        self.energy = None\n",
    "        self.node   = None\n",
    "        self.task   = None\n",
    "        self.target = None\n",
    "        self.action = None\n",
    "    @property\n",
    "    def coordinates(self):\n",
    "        return self.node.coordinates if self.node else None\n",
    "\n",
    "\n",
    "class Fleet:\n",
    "    def __init__(self, team_id):\n",
    "        self.team_id = team_id\n",
    "        self.points  = 0\n",
    "        self.ships   = [Ship(i) for i in range(Global.MAX_UNITS)]\n",
    "\n",
    "    def clear(self):\n",
    "        self.points = 0\n",
    "        for s in self.ships:\n",
    "            s.clean()\n",
    "\n",
    "    def update(self, obs, space):\n",
    "        self.points = int(obs[\"team_points\"][self.team_id])\n",
    "        for ship, active, pos, energy in zip(\n",
    "            self.ships,\n",
    "            obs[\"units_mask\"][self.team_id],\n",
    "            obs[\"units\"][\"position\"][self.team_id],\n",
    "            obs[\"units\"][\"energy\"][self.team_id],\n",
    "        ):\n",
    "            if active:\n",
    "                ship.node   = space.get_node(*pos)\n",
    "                ship.energy = int(energy)\n",
    "                ship.action = None\n",
    "            else:\n",
    "                ship.clean()\n",
    "\n",
    "class Space:\n",
    "    def __init__(self, max_steps):\n",
    "        global FRAME_FACTOR\n",
    "        FRAME_FACTOR = max_steps + 1\n",
    "\n",
    "        self.tile_type       = np.full((SPACE_SIZE,SPACE_SIZE),\n",
    "                                       NodeType.unknown.value,\n",
    "                                       dtype=np.int8)\n",
    "        self.visible         = np.zeros((SPACE_SIZE,SPACE_SIZE), dtype=bool)\n",
    "        self.energy          = np.full((SPACE_SIZE,SPACE_SIZE),\n",
    "                                       -1, dtype=np.int32)\n",
    "\n",
    "        self.relic_mask      = np.zeros((SPACE_SIZE,SPACE_SIZE), dtype=bool)\n",
    "        self.reward_mask     = np.zeros((SPACE_SIZE,SPACE_SIZE), dtype=bool)\n",
    "        self.explored_relic  = np.zeros((SPACE_SIZE,SPACE_SIZE), dtype=bool)\n",
    "        self.explored_reward = np.zeros((SPACE_SIZE,SPACE_SIZE), dtype=bool)\n",
    "\n",
    "        self._nodes = [\n",
    "            [Node(x, y) for x in range(SPACE_SIZE)]\n",
    "            for y in range(SPACE_SIZE)\n",
    "        ]\n",
    "\n",
    "        self.obstacle_map     = np.zeros((SPACE_SIZE,SPACE_SIZE), dtype=bool)\n",
    "        self.obstacle_history = deque(maxlen=100)\n",
    "        self.best_direction   = (0, 0)\n",
    "        self.best_period      = 40\n",
    "        self._movement_counter= 0\n",
    "\n",
    "    def get_node(self, x, y):\n",
    "        return self._nodes[y][x]\n",
    "\n",
    "    def update(self, step, obs, team_id, team_reward):\n",
    "        self._update_map(step, obs)\n",
    "        self._update_relic_map(step, obs, team_id, team_reward)\n",
    "\n",
    "    def _update_map(self, step, obs):\n",
    "        sensor = obs[\"sensor_mask\"]\n",
    "        tiles  = obs[\"map_features\"][\"tile_type\"]\n",
    "        ener   = obs[\"map_features\"][\"energy\"]\n",
    "\n",
    "        self.visible[:]            = sensor\n",
    "        self.tile_type[sensor]     = tiles[sensor]\n",
    "        self.energy   [sensor]     = ener[sensor]\n",
    "        self.energy   [~sensor]    = -1\n",
    "        self.tile_type[OPP_X,OPP_Y]= self.tile_type\n",
    "        self.energy   [OPP_X,OPP_Y]= self.energy\n",
    "        self.visible  [OPP_X,OPP_Y]= self.visible\n",
    "\n",
    "        ys,xs = np.nonzero(sensor)\n",
    "        for x,y in zip(xs,ys):\n",
    "            n = self.get_node(x,y)\n",
    "            n.is_visible = True\n",
    "            n.type       = NodeType(int(self.tile_type[x,y]))\n",
    "            n.energy     = int(self.energy[x,y])\n",
    "            ox,oy = get_opposite(x,y)\n",
    "            m = self.get_node(ox,oy)\n",
    "            m.is_visible = True\n",
    "            m.type       = n.type\n",
    "            m.energy     = n.energy\n",
    "\n",
    "        ys,xs = np.nonzero(~sensor)\n",
    "        for x,y in zip(xs,ys):\n",
    "            n = self.get_node(x,y)\n",
    "            n.is_visible = False\n",
    "            n.energy     = None\n",
    "\n",
    "        self.obstacle_history.append(self.obstacle_map.copy())\n",
    "        self.obstacle_map = (self.tile_type == NodeType.asteroid.value)\n",
    "\n",
    "        if self._movement_counter >= self.best_period:\n",
    "            self._learn_obstacle_drift()\n",
    "            self._movement_counter = 0\n",
    "\n",
    "        self._apply_obstacle_drift(step)\n",
    "\n",
    "    def _learn_obstacle_drift(self):\n",
    "        best_score=-1.0\n",
    "        for per in (10,20,40):\n",
    "            if len(self.obstacle_history) < per: continue\n",
    "            past = self.obstacle_history[-per]\n",
    "            for d in ((1,0),(-1,0),(0,1),(0,-1)):\n",
    "                rolled = np.roll(past, shift=d, axis=(0,1))\n",
    "                score  = float((rolled == self.obstacle_map).mean())\n",
    "                if score > best_score:\n",
    "                    best_score,best_dir,best_per = score,d,per\n",
    "        if best_score > 0.5:\n",
    "            self.best_direction = best_dir\n",
    "            self.best_period    = best_per\n",
    "\n",
    "    def _apply_obstacle_drift(self, step):\n",
    "        if self.best_period <= 0: return\n",
    "        phn = (step / self.best_period) % 1.0\n",
    "        pho = ((step-1) / self.best_period) % 1.0\n",
    "        if pho > phn:\n",
    "            dx,dy = self.best_direction\n",
    "            self.obstacle_map = np.roll(\n",
    "                self.obstacle_map, shift=(dx,dy), axis=(0,1)\n",
    "            )\n",
    "\n",
    "    def _update_relic_map(self, step, obs, team_id, team_reward):\n",
    "        for mask, (x,y) in zip(obs[\"relic_nodes_mask\"], obs[\"relic_nodes\"]):\n",
    "            if mask:\n",
    "                ox,oy = get_opposite(x,y)\n",
    "                for xx,yy in ((x,y),(ox,oy)):\n",
    "                    self.relic_mask[xx,yy]     = True\n",
    "                    self.explored_relic[xx,yy] = True\n",
    "                for nx,ny in self.get_neighbors(x,y, Global.RELIC_REWARD_RANGE):\n",
    "                    self.explored_reward[nx,ny] = False\n",
    "                    self.reward_mask[nx,ny]     = False\n",
    "\n",
    "        newly_seen = self.visible & (~self.explored_relic)\n",
    "        ys,xs = np.nonzero(newly_seen)\n",
    "        for x,y in zip(xs,ys):\n",
    "            ox,oy = get_opposite(x,y)\n",
    "            for xx,yy in ((x,y),(ox,oy)):\n",
    "                self.relic_mask[xx,yy]     = False\n",
    "                self.explored_relic[xx,yy] = True\n",
    "\n",
    "        Global.ALL_RELICS_FOUND  = self.explored_relic.all()\n",
    "        Global.ALL_REWARDS_FOUND = self.explored_reward.all()\n",
    "\n",
    "        match      = step // FRAME_FACTOR\n",
    "        match_step = step % FRAME_FACTOR\n",
    "        num_th     = 2 * min(match, Global.LAST_MATCH_WHEN_RELIC_CAN_APPEAR) + 1\n",
    "        if (not Global.ALL_RELICS_FOUND and self.relic_mask.sum() >= num_th):\n",
    "            Global.ALL_RELICS_FOUND = True\n",
    "            unseen = ~self.explored_relic\n",
    "            ys,xs = np.nonzero(unseen)\n",
    "            for x,y in zip(xs,ys):\n",
    "                ox,oy = get_opposite(x,y)\n",
    "                for xx,yy in ((x,y),(ox,oy)):\n",
    "                    self.relic_mask[xx,yy]     = False\n",
    "                    self.explored_relic[xx,yy] = True\n",
    "\n",
    "        if (not Global.ALL_REWARDS_FOUND and\n",
    "            (match_step > Global.LAST_MATCH_STEP_WHEN_RELIC_CAN_APPEAR\n",
    "             or self.relic_mask.sum() >= num_th)):\n",
    "            self._infer_from_relic_distribution()\n",
    "            self._record_reward_result(obs, team_id, team_reward)\n",
    "            self._infer_reward_status()\n",
    "\n",
    "        for y in range(SPACE_SIZE):\n",
    "            for x in range(SPACE_SIZE):\n",
    "                n = self.get_node(x,y)\n",
    "                n._relic               = bool(self.relic_mask[x,y])\n",
    "                n._explored_for_relic  = bool(self.explored_relic[x,y])\n",
    "                n._reward              = bool(self.reward_mask[x,y])\n",
    "                n._explored_for_reward = bool(self.explored_reward[x,y])\n",
    "\n",
    "    def _infer_from_relic_distribution(self):\n",
    "        mask = (self.relic_mask | ~self.explored_relic).astype(np.int32)\n",
    "        k    = 2 * Global.RELIC_REWARD_RANGE + 1\n",
    "        conv = convolve2d(mask,\n",
    "                          np.ones((k,k),dtype=np.int32),\n",
    "                          mode=\"same\", boundary=\"fill\", fillvalue=0)\n",
    "        ys,xs = np.nonzero(conv == 0)\n",
    "        for x,y in zip(xs,ys):\n",
    "            ox,oy = get_opposite(x,y)\n",
    "            for xx,yy in ((x,y),(ox,oy)):\n",
    "                self.reward_mask[xx,yy]     = False\n",
    "                self.explored_reward[xx,yy] = True\n",
    "\n",
    "    def _record_reward_result(self, obs, team_id, team_reward):\n",
    "        ship_positions = set()\n",
    "        for active, energy, pos in zip(\n",
    "            obs[\"units_mask\"][team_id],\n",
    "            obs[\"units\"][\"energy\"][team_id],\n",
    "            obs[\"units\"][\"position\"][team_id],\n",
    "        ):\n",
    "            if active and energy >= 0:\n",
    "                ship_positions.add(tuple(pos))\n",
    "        Global.REWARD_RESULTS.append({\n",
    "            \"nodes\":  ship_positions,\n",
    "            \"reward\": int(team_reward)\n",
    "        })\n",
    "\n",
    "    def _infer_reward_status(self):\n",
    "        for result in Global.REWARD_RESULTS:\n",
    "            known = 0\n",
    "            unknown = set()\n",
    "            for (x,y) in result[\"nodes\"]:\n",
    "                if self.explored_reward[x,y] and not self.reward_mask[x,y]:\n",
    "                    continue\n",
    "                if self.reward_mask[x,y]:\n",
    "                    known += 1\n",
    "                else:\n",
    "                    unknown.add((x,y))\n",
    "            if not unknown:\n",
    "                continue\n",
    "            rem = result[\"reward\"] - known\n",
    "            if rem == 0:\n",
    "                for x,y in unknown:\n",
    "                    ox,oy = get_opposite(x,y)\n",
    "                    for xx,yy in ((x,y),(ox,oy)):\n",
    "                        self.reward_mask[xx,yy]     = False\n",
    "                        self.explored_reward[xx,yy] = True\n",
    "            elif rem == len(unknown):\n",
    "                for x,y in unknown:\n",
    "                    ox,oy = get_opposite(x,y)\n",
    "                    for xx,yy in ((x,y),(ox,oy)):\n",
    "                        self.reward_mask[xx,yy]     = True\n",
    "                        self.explored_reward[xx,yy] = True\n",
    "\n",
    "    def get_neighbors(self, x, y, r):\n",
    "        for dx,dy in NEIGHBOR_OFFSETS[r]:\n",
    "            yield warp_point(x+dx, y+dy)\n",
    "\n",
    "TARGET_UPDATE = 4000\n",
    "TAU_SOFT      = 0.005\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, player: str, env_cfg: dict, training: bool = True) -> None:\n",
    "        self.player       = player\n",
    "        self.team_id      = 0 if player == \"player_0\" else 1\n",
    "        self.opp_team_id  = 1 - self.team_id\n",
    "        self.env_cfg      = env_cfg\n",
    "        self.training     = training\n",
    "    \n",
    "        self.map_width    = env_cfg[\"map_width\"]\n",
    "        self.map_height   = env_cfg[\"map_height\"]\n",
    "        self.max_steps    = env_cfg[\"max_steps_in_match\"]\n",
    "        self.UNIT_SAP_RANGE = env_cfg[\"unit_sap_range\"]\n",
    "    \n",
    "        self.num_matches = 5\n",
    "    \n",
    "        self.space       = Space(self.max_steps)\n",
    "        self.fleet       = Fleet(self.team_id)\n",
    "        self.opp_fleet   = Fleet(self.opp_team_id)\n",
    "    \n",
    "        self.local_view_size = 11\n",
    "        self.input_channels  = 9 + self.num_matches\n",
    "        self.action_size     = 5\n",
    "        self.hidden_size     = 512\n",
    "        self.batch_size      = 128\n",
    "        self.gamma           = 0.99\n",
    "        self.epsilon_start   = 1.0\n",
    "        self.epsilon_min     = 0.15\n",
    "        self.epsilon_decay   = 0.9998\n",
    "        self.learning_rate   = 1e-4\n",
    "        self.replay_capacity = 50_000\n",
    "        self.beta_frames     = 200_000\n",
    "        self.frame_count     = 1\n",
    "    \n",
    "        self.device     = device\n",
    "        self.policy_net = DQN(\n",
    "            self.input_channels,\n",
    "            self.action_size,\n",
    "            self.local_view_size,\n",
    "            self.hidden_size\n",
    "        ).to(self.device)\n",
    "        self.target_net = DQN(\n",
    "            self.input_channels,\n",
    "            self.action_size,\n",
    "            self.local_view_size,\n",
    "            self.hidden_size\n",
    "        ).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "    \n",
    "        self.optimizer = optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=10_000, gamma=0.1)\n",
    "        self.memory    = ReplayBuffer(self.replay_capacity,\n",
    "                                      beta_frames=self.beta_frames)\n",
    "        self.epsilon   = self.epsilon_start if training else 0.0\n",
    "    \n",
    "        self.visit_counts = np.zeros((self.map_width, self.map_height), dtype=int)\n",
    "\n",
    "        if not training:\n",
    "            self.load_model()\n",
    "\n",
    "    def _state_representation(self, unit_pos, unit_energy, step, obs):\n",
    "        L = self.local_view_size\n",
    "        R = L // 2\n",
    "        xs = (np.arange(-R, R+1) + unit_pos[0]) % self.map_width\n",
    "        ys = (np.arange(-R, R+1) + unit_pos[1]) % self.map_height\n",
    "    \n",
    "        tile_v   = self.space.tile_type[np.ix_(xs, ys)].astype(np.float32) / 3.0\n",
    "        en_v     = self.space.energy   [np.ix_(xs, ys)].astype(np.float32) / 100.0\n",
    "        relic_v  = self.space.relic_mask   [np.ix_(xs, ys)].astype(np.float32)\n",
    "        reward_v = self.space.reward_mask  [np.ix_(xs, ys)].astype(np.float32)\n",
    "        explored_rel = self.space.explored_relic[np.ix_(xs, ys)]\n",
    "    \n",
    "        units_v = np.zeros((L, L), dtype=np.float32)\n",
    "        for ship in self.fleet.ships:\n",
    "            if ship.node:\n",
    "                dx = (ship.node.x - unit_pos[0]) % self.map_width\n",
    "                dy = (ship.node.y - unit_pos[1]) % self.map_height\n",
    "                if dx <= R or dx >= self.map_width - R - 1:\n",
    "                    if dy <= R or dy >= self.map_height - R - 1:\n",
    "                        xi, yi = (dx + R) % L, (dy + R) % L\n",
    "                        units_v[xi, yi] = ship.energy / 100.0\n",
    "        for opp in self.opp_fleet.ships:\n",
    "            if opp.node:\n",
    "                dx = (opp.node.x - unit_pos[0]) % self.map_width\n",
    "                dy = (opp.node.y - unit_pos[1]) % self.map_height\n",
    "                if dx <= R or dx >= self.map_width - R - 1:\n",
    "                    if dy <= R or dy >= self.map_height - R - 1:\n",
    "                        xi, yi = (dx + R) % L, (dy + R) % L\n",
    "                        units_v[xi, yi] = -opp.energy / 100.0\n",
    "    \n",
    "        dist_k    = np.fromfunction(lambda i,j: np.abs(i-R)+np.abs(j-R), (L,L), dtype=np.float32)\n",
    "        mask_unexp= (~explored_rel).astype(np.float32)\n",
    "        dr        = convolve2d(mask_unexp, np.ones((1,1)), mode=\"same\", boundary=\"wrap\")\n",
    "        dist_relic = np.where(dr == 0, 1.0, dist_k / (self.map_width + self.map_height))\n",
    "    \n",
    "        mask_rw    = reward_v.astype(bool)\n",
    "        dist_reward= np.ones((L,L), dtype=np.float32)\n",
    "        denom      = self.map_width + self.map_height\n",
    "        I,J        = np.indices((L,L))\n",
    "        rows, cols = np.where(mask_rw)\n",
    "        for r,c in zip(rows, cols):\n",
    "            d = (np.abs(I-r) + np.abs(J-c)) / denom\n",
    "            dist_reward = np.minimum(dist_reward, d)\n",
    "    \n",
    "        ue = np.full((L,L), unit_energy/100.0, dtype=np.float32)\n",
    "        sp = np.full((L,L), step/FRAME_FACTOR,    dtype=np.float32)\n",
    "    \n",
    "        base = np.stack([\n",
    "            tile_v, en_v, relic_v, reward_v,\n",
    "            units_v, ue, sp,\n",
    "            dist_relic, dist_reward\n",
    "        ], axis=0)\n",
    "    \n",
    "        match = step // FRAME_FACTOR\n",
    "        phase_oh = np.zeros((self.num_matches, L, L), dtype=np.float32)\n",
    "        if match < self.num_matches:\n",
    "            phase_oh[match, :, :] = 1.0\n",
    "    \n",
    "        state_np = np.concatenate([base, phase_oh], axis=0)\n",
    "        return torch.from_numpy(state_np).float().to(self.device)\n",
    "\n",
    "    def _get_action_mask(self, unit_pos):\n",
    "        mask = np.ones(self.action_size, dtype=np.int8)\n",
    "        x,y = unit_pos\n",
    "        if x == 0:                mask[ActionType.left]  = 0\n",
    "        if x == self.map_width-1: mask[ActionType.right] = 0\n",
    "        if y == 0:                mask[ActionType.up]    = 0\n",
    "        if y == self.map_height-1:mask[ActionType.down]  = 0\n",
    "        return mask\n",
    "\n",
    "    def try_sap_on_enemies(self, ship):\n",
    "        targets = []\n",
    "        for opp in self.opp_fleet.ships:\n",
    "            if opp.node:\n",
    "                d = manhattan_distance(ship.coordinates, opp.coordinates)\n",
    "                if d <= self.UNIT_SAP_RANGE:\n",
    "                    targets.append((opp.energy - d, opp))\n",
    "        if not targets:\n",
    "            return False\n",
    "        _, best = max(targets, key=lambda x: x[0])\n",
    "        dx = best.coordinates[0] - ship.coordinates[0]\n",
    "        dy = best.coordinates[1] - ship.coordinates[1]\n",
    "        ship.action = (ActionType.sap, dx, dy)\n",
    "        return True\n",
    "\n",
    "    def act(self, step, obs, remainingOverageTime=60):\n",
    "        pts = int(obs[\"team_points\"][self.team_id])\n",
    "        rew = max(0, pts - self.fleet.points)\n",
    "\n",
    "        self.space.update(step, obs, self.team_id, rew)\n",
    "        self.fleet.update(obs, self.space)\n",
    "        self.opp_fleet.update(obs, self.space)\n",
    "\n",
    "        for ship in self.fleet.ships:\n",
    "            if ship.node:\n",
    "                x,y = ship.coordinates\n",
    "                self.visit_counts[x,y] += 1\n",
    "\n",
    "        acts = []\n",
    "        for ship in self.fleet.ships:\n",
    "            if ship.node is None:\n",
    "                acts.append((0,0,0))\n",
    "                continue\n",
    "            if self.try_sap_on_enemies(ship):\n",
    "                acts.append(ship.action)\n",
    "                continue\n",
    "\n",
    "            state = self._state_representation(\n",
    "                ship.coordinates, ship.energy, step, obs\n",
    "            )\n",
    "            mask  = self._get_action_mask(ship.coordinates)\n",
    "\n",
    "            if random.random() < self.epsilon and self.training:\n",
    "                valid = np.where(mask==1)[0]\n",
    "                a     = int(random.choice(valid)) if valid.size else 0\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q = self.policy_net(state.unsqueeze(0))\n",
    "                    mask_t = torch.tensor(mask, dtype=torch.float32, device=self.device)\n",
    "                    inv    = (1.0 - mask_t) * -1e9\n",
    "                    a      = int((q + inv).argmax().item())\n",
    "\n",
    "            acts.append((a,0,0))\n",
    "\n",
    "        return np.array(acts)\n",
    "\n",
    "    def learn(self):\n",
    "        if not self.training or len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        batch, idxs, wts = self.memory.sample(self.batch_size)\n",
    "        self.frame_count += 1\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states      = torch.stack(states).to(self.device)\n",
    "        next_states = torch.stack(next_states).to(self.device)\n",
    "        actions     = torch.as_tensor(actions, dtype=torch.long, device=self.device)\\\n",
    "                          .clamp(0, self.action_size-1)\n",
    "        rewards     = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones       = torch.tensor([float(d) for d in dones],\n",
    "                                  dtype=torch.float32, device=self.device)\n",
    "        weights     = wts.unsqueeze(1)\n",
    "\n",
    "        q_vals = self.policy_net(states)\n",
    "        q_sa   = q_vals.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_a = self.policy_net(next_states).argmax(1, keepdim=True)\n",
    "            next_q = self.target_net(next_states).gather(1, next_a)\n",
    "            target = rewards.unsqueeze(1) + (1.0 - dones.unsqueeze(1)) * self.gamma * next_q\n",
    "\n",
    "        td_error = target - q_sa\n",
    "        self.memory.update_priorities(idxs, td_error.abs().detach().cpu().numpy())\n",
    "\n",
    "        loss = (weights * F.smooth_l1_loss(q_sa, target, reduction=\"none\")).mean()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        if TAU_SOFT is not None:\n",
    "            for tp, pp in zip(self.target_net.parameters(),\n",
    "                              self.policy_net.parameters()):\n",
    "                tp.data.mul_(1-TAU_SOFT).add_(TAU_SOFT * pp.data)\n",
    "        elif self.frame_count % TARGET_UPDATE == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min,\n",
    "                           self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return {\n",
    "            \"loss\":    loss.item(),\n",
    "            \"td_mean\": td_error.abs().mean().item(),\n",
    "            \"lr\":      self.optimizer.param_groups[0][\"lr\"],\n",
    "            \"eps\":     self.epsilon,\n",
    "            \"frame\":   self.frame_count,\n",
    "        }\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save({\n",
    "            \"policy_net\": self.policy_net.state_dict(),\n",
    "            \"target_net\": self.target_net.state_dict(),\n",
    "            \"optimizer\":  self.optimizer.state_dict(),\n",
    "            \"frame_count\":self.frame_count,\n",
    "            \"epsilon\":    self.epsilon,\n",
    "        }, f\"dqn_{self.player}.pth\")\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            ckpt = torch.load(f\"dqn_{self.player}.pth\",\n",
    "                              map_location=\"cpu\", weights_only=True)\n",
    "            self.policy_net.load_state_dict(ckpt[\"policy_net\"])\n",
    "            self.target_net.load_state_dict(ckpt[\"target_net\"])\n",
    "            self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "            self.frame_count = ckpt.get(\"frame_count\", self.frame_count)\n",
    "            self.epsilon     = ckpt.get(\"epsilon\", self.epsilon)\n",
    "            self.target_net.eval()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "class SimpleAgent:\n",
    "    def __init__(self, player: str, env_cfg: dict, training: bool = False):\n",
    "        self.player  = player\n",
    "        self.team_id = 0 if player==\"player_0\" else 1\n",
    "        self.env_cfg = env_cfg\n",
    "\n",
    "    def act(self, step: int, obs: dict, remainingOverageTime: int = 60):\n",
    "        actions = []\n",
    "        for uid in range(self.env_cfg[\"max_units\"]):\n",
    "            if obs[\"units_mask\"][self.team_id][uid]:\n",
    "                x,y = obs[\"units\"][\"position\"][self.team_id][uid]\n",
    "                moves=[]\n",
    "                if x>0: moves.append(ActionType.left)\n",
    "                if x<self.env_cfg[\"map_width\"]-1: moves.append(ActionType.right)\n",
    "                if y>0: moves.append(ActionType.up)\n",
    "                if y<self.env_cfg[\"map_height\"]-1: moves.append(ActionType.down)\n",
    "                moves.append(ActionType.center)\n",
    "                a = random.choice(moves).value\n",
    "                actions.append((a,0,0))\n",
    "            else:\n",
    "                actions.append((0,0,0))\n",
    "        return np.array(actions)\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, player: str, env_cfg: dict, training: bool = False):\n",
    "        self.player  = player\n",
    "        self.team_id = 0 if player==\"player_0\" else 1\n",
    "        self.env_cfg = env_cfg\n",
    "        self._seen_relic_ids  = set()\n",
    "        self._relic_positions = []\n",
    "        self._explore_targets = {}\n",
    "\n",
    "    def act(self, step: int, obs: dict, remainingOverageTime: int = 60):\n",
    "        max_u = self.env_cfg[\"max_units\"]\n",
    "        actions = np.zeros((max_u,3), dtype=int)\n",
    "        unit_mask = obs[\"units_mask\"][self.team_id]\n",
    "        unit_pos  = obs[\"units\"][\"position\"][self.team_id]\n",
    "        relic_mask= obs[\"relic_nodes_mask\"]\n",
    "        relics    = obs[\"relic_nodes\"]\n",
    "\n",
    "        for ridx, seen in enumerate(relic_mask):\n",
    "            if seen and ridx not in self._seen_relic_ids:\n",
    "                self._seen_relic_ids.add(ridx)\n",
    "                self._relic_positions.append(tuple(relics[ridx]))\n",
    "\n",
    "        for u in range(max_u):\n",
    "            if not unit_mask[u]:\n",
    "                continue\n",
    "            src = tuple(unit_pos[u])\n",
    "            if self._relic_positions:\n",
    "                dest = self._relic_positions[0]\n",
    "                dx,dy = dest[0]-src[0], dest[1]-src[1]\n",
    "                dist  = abs(dx)+abs(dy)\n",
    "                if dist<=4:\n",
    "                    move = np.random.randint(0,5)\n",
    "                else:\n",
    "                    if abs(dx)>abs(dy):\n",
    "                        move = ActionType.right.value if dx>0 else ActionType.left.value\n",
    "                    else:\n",
    "                        move = ActionType.down.value  if dy>0 else ActionType.up.value\n",
    "            else:\n",
    "                if (step%20==0) or (u not in self._explore_targets):\n",
    "                    tx = np.random.randint(0,self.env_cfg[\"map_width\"])\n",
    "                    ty = np.random.randint(0,self.env_cfg[\"map_height\"])\n",
    "                    self._explore_targets[u] = (tx,ty)\n",
    "                dest = self._explore_targets[u]\n",
    "                dx,dy = dest[0]-src[0], dest[1]-src[1]\n",
    "                if (dx,dy)==(0,0):\n",
    "                    move = ActionType.center.value\n",
    "                else:\n",
    "                    if abs(dx)>abs(dy):\n",
    "                        move = ActionType.right.value if dx>0 else ActionType.left.value\n",
    "                    else:\n",
    "                        move = ActionType.down.value  if dy>0 else ActionType.up.value\n",
    "\n",
    "            actions[u] = (move,0,0)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class CollectorAgent:\n",
    "    def __init__(self, player: str, env_cfg: dict, training: bool = False):\n",
    "        self.player    = player\n",
    "        self.team_id   = 0 if player==\"player_0\" else 1\n",
    "        self.env_cfg   = env_cfg\n",
    "        self.training  = training\n",
    "        self.neighbors = [] \n",
    "\n",
    "    def act(self, step: int, obs: dict, remainingOverageTime: int = 60):\n",
    "        for mask, (rx, ry) in zip(obs[\"relic_nodes_mask\"], obs[\"relic_nodes\"]):\n",
    "            if mask:\n",
    "                self.neighbors = [\n",
    "                    warp_point(rx + dx, ry + dy)\n",
    "                    for dx, dy in NEIGHBOR_OFFSETS[2]\n",
    "                ]\n",
    "                break\n",
    "\n",
    "        actions = np.zeros((self.env_cfg[\"max_units\"], 3), dtype=int)\n",
    "\n",
    "        for uid in range(self.env_cfg[\"max_units\"]):\n",
    "            if not obs[\"units_mask\"][self.team_id][uid]:\n",
    "                continue\n",
    "            x, y = obs[\"units\"][\"position\"][self.team_id][uid]\n",
    "            if self.neighbors:\n",
    "                src = np.array((x, y))\n",
    "                tgt = np.array(random.choice(self.neighbors))\n",
    "                dir_idx = direction_to(src, tgt)\n",
    "                actions[uid] = (int(dir_idx), 0, 0)\n",
    "            else:\n",
    "                actions[uid] = (random.randint(0, 4), 0, 0)\n",
    "\n",
    "        return actions\n",
    "\n",
    "import json\n",
    "from IPython.display import display, Javascript\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "\n",
    "def render_episode(episode: RecordEpisode) -> None:\n",
    "    data = json.dumps(episode.serialize_episode_data(), separators=(\",\", \":\"))\n",
    "    display(Javascript(f\"\"\"\n",
    "var iframe = document.createElement('iframe');\n",
    "iframe.src = 'https://s3vis.lux-ai.org/#/kaggle';\n",
    "iframe.width = '100%';\n",
    "iframe.scrolling = 'no';\n",
    "\n",
    "iframe.addEventListener('load', event => {{\n",
    "    event.target.contentWindow.postMessage({data}, 'https://s3vis.lux-ai.org');\n",
    "}});\n",
    "\n",
    "new ResizeObserver(entries => {{\n",
    "    for (const entry of entries) {{\n",
    "        entry.target.height = `${{Math.round(320 + 0.3 * entry.contentRect.width)}}px`;\n",
    "    }}\n",
    "}}).observe(iframe);\n",
    "\n",
    "element.append(iframe);\n",
    "    \"\"\"))\n",
    "\n",
    "def validate(agent_0, agent_1, env_cfg, num_games=3):\n",
    "    env = LuxAIS3GymEnv(numpy_output=True)\n",
    "    total_r0 = 0.0\n",
    "    total_r1 = 0.0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        obs, _ = env.reset(seed=random.randint(0, 100_000))\n",
    "        done = False\n",
    "        step = 0\n",
    "        r0 = 0.0\n",
    "        r1 = 0.0\n",
    "\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for ag in (agent_0, agent_1):\n",
    "                if obs[ag.player] is not None:\n",
    "                    actions[ag.player] = ag.act(step, obs[ag.player])\n",
    "                else:\n",
    "                    actions[ag.player] = np.zeros((env_cfg[\"max_units\"], 3), dtype=int)\n",
    "\n",
    "            obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "\n",
    "            r0 = max(r0, rewards.get(\"player_0\", 0.0))\n",
    "            r1 = max(r1, rewards.get(\"player_1\", 0.0))\n",
    "                          \n",
    "            done = (all(terminated.values()) or all(truncated.values()))\n",
    "            step += 1\n",
    "            \n",
    "        total_r0 += r0\n",
    "        total_r1 += r1\n",
    "\n",
    "    env.close()\n",
    "    return total_r0, total_r1\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_agents(agent_cls_p0, agent_cls_p1,\n",
    "                    seed=42, training=True,\n",
    "                    games_to_play=500, validate_every=10):\n",
    "    wandb.init(\n",
    "        project=\"dqn_luxai_s3_gpu\",\n",
    "        config={\n",
    "            \"seed\": seed,\n",
    "            \"games_to_play\": games_to_play,\n",
    "            \"validate_every\": validate_every,\n",
    "            \"batch_size\": 128,\n",
    "            \"gamma\": 0.99,\n",
    "            \"epsilon_start\": 1.0,\n",
    "            \"epsilon_min\": 0.05,\n",
    "            \"epsilon_decay\": 0.9995,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"replay_buffer_capacity\": 50_000,\n",
    "            \"beta_frames\": 200_000,\n",
    "        }\n",
    "    )\n",
    "    cfg = wandb.config\n",
    "\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "    env = LuxAIS3GymEnv(numpy_output=True)\n",
    "    obs, info = env.reset(seed=cfg.seed)\n",
    "    env_cfg = info[\"params\"]\n",
    "\n",
    "    p0 = agent_cls_p0(\"player_0\", env_cfg, training=training)\n",
    "    p1 = agent_cls_p1(\"player_1\", env_cfg, training=training)\n",
    "\n",
    "    p0.policy_net.to(device); p0.target_net.to(device)\n",
    "    p1.policy_net.to(device); p1.target_net.to(device)\n",
    "\n",
    "    for ep in range(games_to_play):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        done     = False\n",
    "        step     = 0\n",
    "        last_obs = None\n",
    "        last_act = None\n",
    "        ep_r0 = ep_r1 = 0.0\n",
    "\n",
    "        p0_losses    = []\n",
    "        p0_td_means  = []\n",
    "        p1_losses    = []\n",
    "        p1_td_means  = []\n",
    "\n",
    "        while not done:\n",
    "            if training:\n",
    "                last_obs = {\n",
    "                    \"player_0\": obs[\"player_0\"].copy() if obs[\"player_0\"] else None,\n",
    "                    \"player_1\": obs[\"player_1\"].copy() if obs[\"player_1\"] else None,\n",
    "                }\n",
    "\n",
    "            actions = {}\n",
    "            for ag in (p0, p1):\n",
    "                if obs[ag.player] is not None:\n",
    "                    actions[ag.player] = ag.act(step, obs[ag.player])\n",
    "                else:\n",
    "                    actions[ag.player] = np.zeros((env_cfg[\"max_units\"],3), dtype=int)\n",
    "            if training:\n",
    "                last_act = actions.copy()\n",
    "\n",
    "            obs, rewards, term, trunc, _ = env.step(actions)\n",
    "            dones = {\n",
    "                \"player_0\": term[\"player_0\"] or trunc[\"player_0\"],\n",
    "                \"player_1\": term[\"player_1\"] or trunc[\"player_1\"],\n",
    "            }\n",
    "            done = all(term.values()) or all(trunc.values())\n",
    "\n",
    "            ep_r0 = max(ep_r0, rewards.get(\"player_0\", 0.0))\n",
    "            ep_r1 = max(ep_r1, rewards.get(\"player_1\", 0.0))\n",
    "\n",
    "            if training and last_obs is not None:\n",
    "                match = step // FRAME_FACTOR\n",
    "\n",
    "                for ag in (p0, p1):\n",
    "                    tid = ag.team_id\n",
    "                    for uid in range(env_cfg[\"max_units\"]):\n",
    "                        if not (last_obs[ag.player]\n",
    "                                and last_obs[ag.player][\"units_mask\"][tid][uid]):\n",
    "                            continue\n",
    "\n",
    "                        s = ag._state_representation(\n",
    "                            last_obs[ag.player][\"units\"][\"position\"][tid][uid],\n",
    "                            last_obs[ag.player][\"units\"][\"energy\"][tid][uid],\n",
    "                            step,\n",
    "                            last_obs[ag.player]\n",
    "                        )\n",
    "                        if obs[ag.player] and obs[ag.player][\"units_mask\"][tid][uid]:\n",
    "                            s_next = ag._state_representation(\n",
    "                                obs[ag.player][\"units\"][\"position\"][tid][uid],\n",
    "                                obs[ag.player][\"units\"][\"energy\"][tid][uid],\n",
    "                                step + 1,\n",
    "                                obs[ag.player]\n",
    "                            )\n",
    "                        else:\n",
    "                            s_next = torch.zeros_like(s)\n",
    "\n",
    "                        base_r = 0.0\n",
    "                        a      = last_act[ag.player][uid][0]\n",
    "\n",
    "                        x,y = last_obs[ag.player][\"units\"][\"position\"][tid][uid]\n",
    "                        node = ag.space.get_node(x, y)\n",
    "                        explore_bonus  = 0.0\n",
    "                        if not node.explored_for_relic:\n",
    "                            explore_bonus += 5.0\n",
    "                        if not node.explored_for_reward:\n",
    "                            explore_bonus += 2.5\n",
    "                        vis = ag.visit_counts[x, y]\n",
    "                        explore_bonus += (10.0 / np.sqrt(vis)) if vis > 0 else 0.5\n",
    "\n",
    "                        exploit_bonus = 6.0 if node.reward else 0.0\n",
    "\n",
    "                        if match < 2:\n",
    "                            intrinsic = explore_bonus * 1.5 + exploit_bonus * 0.5\n",
    "                        else:\n",
    "                            intrinsic = exploit_bonus * 1.5 + explore_bonus * 0.125\n",
    "\n",
    "                        if (step + 1) % FRAME_FACTOR == 0:\n",
    "                            intrinsic += 1.0 \n",
    "\n",
    "                        r_shaped = (base_r + intrinsic)\n",
    "\n",
    "                        ag.memory.push(s, a, r_shaped, s_next, dones[ag.player])\n",
    "\n",
    "                        s_np    = s.cpu().numpy()\n",
    "                        s_flip  = s_np[:, ::-1, ::-1].copy()\n",
    "                        s_flip_t= torch.from_numpy(s_flip).to(ag.device)\n",
    "                        ns_np   = s_next.cpu().numpy()\n",
    "                        ns_flip = ns_np[:, ::-1, ::-1].copy()\n",
    "                        ns_flip_t= torch.from_numpy(ns_flip).to(ag.device)\n",
    "                        a_flip  = MIRROR_ACTION.get(a, a)\n",
    "                        ag.memory.push(s_flip_t, a_flip, r_shaped, ns_flip_t, dones[ag.player])\n",
    "\n",
    "                stats0 = p0.learn()\n",
    "                stats1 = p1.learn()\n",
    "                if stats0:\n",
    "                    p0_losses.append(stats0[\"loss\"])\n",
    "                    p0_td_means.append(stats0[\"td_mean\"])\n",
    "                if stats1:\n",
    "                    p1_losses.append(stats1[\"loss\"])\n",
    "                    p1_td_means.append(stats1[\"td_mean\"])\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        log_data = {\n",
    "            \"episode\":    ep,\n",
    "            \"ep_reward_p0\": ep_r0,\n",
    "            \"ep_reward_p1\": ep_r1,\n",
    "        }\n",
    "        if p0_losses:\n",
    "            log_data.update({\n",
    "                \"avg_loss_p0\":   np.mean(p0_losses),\n",
    "                \"avg_td_p0\":     np.mean(p0_td_means),\n",
    "                \"final_eps_p0\":  p0.epsilon,\n",
    "            })\n",
    "        if p1_losses:\n",
    "            log_data.update({\n",
    "                \"avg_loss_p1\":   np.mean(p1_losses),\n",
    "                \"avg_td_p1\":     np.mean(p1_td_means),\n",
    "                \"final_eps_p1\":  p1.epsilon,\n",
    "            })\n",
    "\n",
    "        for ag in (p0, p1):\n",
    "            if ag.epsilon == ag.epsilon_min:\n",
    "                ag.epsilon = 0.2\n",
    "            else:\n",
    "                ag.epsilon = min(\n",
    "                    ag.epsilon_start,\n",
    "                    ag.epsilon * (ag.epsilon_decay ** (-(FRAME_FACTOR // 2)))\n",
    "                )\n",
    "\n",
    "        wandb.log(log_data)\n",
    "\n",
    "        if training and (ep + 1) % validate_every == 0:\n",
    "            baseAgent1 = BaseAgent(\"player_1\", env_cfg, training=False)\n",
    "            val_p0_vsBase, _ = validate(p0, baseAgent1, env_cfg)\n",
    "            baseAgent0 = BaseAgent(\"player_0\", env_cfg, training=False)\n",
    "            _, val_p1_vsBase = validate(baseAgent0, p1, env_cfg)\n",
    "            wandb.log({\n",
    "                \"val_p0_vsBase\": val_p0_vsBase,\n",
    "                \"val_p1_vsBase\": val_p1_vsBase,\n",
    "            })\n",
    "\n",
    "        if training and (ep + 1) % validate_every == 0:\n",
    "            p0.save_model()\n",
    "            p1.save_model()\n",
    "\n",
    "        if training and (ep + 1) % (50) == 0:\n",
    "            # record one match\n",
    "            rec_env = RecordEpisode(\n",
    "                LuxAIS3GymEnv(numpy_output=True),\n",
    "                save_on_close=False,\n",
    "                save_on_reset=False,\n",
    "                save_dir=None\n",
    "            )\n",
    "            obs_r, _ = rec_env.reset(seed=cfg.seed + ep)\n",
    "            done_r, step_r = False, 0\n",
    "            baseAgent = BaseAgent(\"player_1\", env_cfg, training=False)\n",
    "            while not done_r:\n",
    "                acts = {}\n",
    "                for ag in (p0, baseAgent):\n",
    "                    if obs_r[ag.player] is not None:\n",
    "                        acts[ag.player] = ag.act(step_r, obs_r[ag.player])\n",
    "                    else:\n",
    "                        acts[ag.player] = np.zeros((env_cfg[\"max_units\"],3), dtype=int)\n",
    "                obs_r, _, term_r, trunc_r, _ = rec_env.step(acts)\n",
    "                done_r = term_r[\"player_0\"] or trunc_r[\"player_0\"] or term_r[\"player_1\"] or trunc_r[\"player_1\"]\n",
    "                step_r += 1\n",
    "\n",
    "            render_episode(rec_env)\n",
    "            rec_env.close()\n",
    "\n",
    "    env.close()\n",
    "    if training:\n",
    "        p0.save_model()\n",
    "        p1.save_model()\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_agents(\n",
    "        agent_cls_p0=Agent,\n",
    "        agent_cls_p1=Agent,\n",
    "        seed=42,\n",
    "        training=True,\n",
    "        games_to_play=50,\n",
    "        validate_every=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756f852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Javascript\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "\n",
    "def render_episode(episode: RecordEpisode) -> None:\n",
    "    data = json.dumps(episode.serialize_episode_data(), separators=(\",\", \":\"))\n",
    "    display(Javascript(f\"\"\"\n",
    "var iframe = document.createElement('iframe');\n",
    "iframe.src = 'https://s3vis.lux-ai.org/#/kaggle';\n",
    "iframe.width = '100%';\n",
    "iframe.scrolling = 'no';\n",
    "\n",
    "iframe.addEventListener('load', event => {{\n",
    "    event.target.contentWindow.postMessage({data}, 'https://s3vis.lux-ai.org');\n",
    "}});\n",
    "\n",
    "new ResizeObserver(entries => {{\n",
    "    for (const entry of entries) {{\n",
    "        entry.target.height = `${{Math.round(320 + 0.3 * entry.contentRect.width)}}px`;\n",
    "    }}\n",
    "}}).observe(iframe);\n",
    "\n",
    "element.append(iframe);\n",
    "    \"\"\"))\n",
    "\n",
    "def evaluate_agents(agent_1_cls, agent_2_cls, seed=42, games_to_play=5, replay_save_dir=\"replays\"):\n",
    "    env = RecordEpisode(\n",
    "        LuxAIS3GymEnv(numpy_output=True), save_on_close=True, save_on_reset=True, save_dir=replay_save_dir\n",
    "    )\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    for i in range(games_to_play):\n",
    "        obs, info = env.reset()\n",
    "        env_cfg = info[\"params\"] # only contains observable game parameters\n",
    "        player_0 = agent_1_cls(\"player_0\", env_cfg)\n",
    "        player_1 = agent_2_cls(\"player_1\", env_cfg)\n",
    "    \n",
    "        # main game loop\n",
    "        game_done = False\n",
    "        step = 0\n",
    "        print(f\"Running game {i}\")\n",
    "        while not game_done:\n",
    "            actions = dict()\n",
    "            for agent in [player_0, player_1]:\n",
    "                actions[agent.player] = agent.act(step=step, obs=obs[agent.player])\n",
    "            obs, reward, terminated, truncated, info = env.step(actions)\n",
    "\n",
    "            dones = {k: terminated[k] | truncated[k] for k in terminated}\n",
    "            if dones[\"player_0\"] or dones[\"player_1\"]:\n",
    "                game_done = True\n",
    "            step += 1\n",
    "        render_episode(env)\n",
    "    env.close() # free up resources and save final replay\n",
    "\n",
    "evaluate_agents(BaseAgent, Agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
