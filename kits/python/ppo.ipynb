{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1158d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from enum import IntEnum\n",
    "\n",
    "# taken from https://www.kaggle.com/code/yizhewang3/ppo-stable-baselines3\n",
    "\n",
    "SPACE_SIZE = 24\n",
    "NUM_TEAMS = 2\n",
    "MAX_UNITS = 16\n",
    "RELIC_REWARD_RANGE = 2\n",
    "MAX_STEPS_IN_MATCH = 100\n",
    "MAX_ENERGY_PER_TILE = 20\n",
    "MAX_RELIC_NODES = 6\n",
    "LAST_MATCH_STEP_WHEN_RELIC_CAN_APPEAR = 50\n",
    "LAST_MATCH_WHEN_RELIC_CAN_APPEAR = 2\n",
    "\n",
    "# We will find the exact value of these constants during the game\n",
    "UNIT_MOVE_COST = 1  # OPTIONS: list(range(1, 6))\n",
    "UNIT_SAP_COST = 30  # OPTIONS: list(range(30, 51))\n",
    "UNIT_SAP_RANGE = 3  # OPTIONS: list(range(3, 8))\n",
    "UNIT_SENSOR_RANGE = 2  # OPTIONS: [1, 2, 3, 4]\n",
    "OBSTACLE_MOVEMENT_PERIOD = 20  # OPTIONS: 6.67, 10, 20, 40\n",
    "OBSTACLE_MOVEMENT_DIRECTION = (0, 0)  # OPTIONS: [(1, -1), (-1, 1)]\n",
    "\n",
    "# We will NOT find the exact value of these constants during the game\n",
    "NEBULA_ENERGY_REDUCTION = 5  # OPTIONS: [0, 1, 2, 3, 5, 25]\n",
    "\n",
    "# Exploration flags:\n",
    "\n",
    "ALL_RELICS_FOUND = False\n",
    "ALL_REWARDS_FOUND = False\n",
    "OBSTACLE_MOVEMENT_PERIOD_FOUND = False\n",
    "OBSTACLE_MOVEMENT_DIRECTION_FOUND = False\n",
    "\n",
    "# Game logs:\n",
    "\n",
    "# REWARD_RESULTS: [{\"nodes\": Set[Node], \"points\": int}, ...]\n",
    "# A history of reward events, where each entry contains:\n",
    "# - \"nodes\": A set of nodes where our ships were located.\n",
    "# - \"points\": The number of points scored at that location.\n",
    "# This data will help identify which nodes yield points.\n",
    "REWARD_RESULTS = []\n",
    "\n",
    "# obstacles_movement_status: list of bool\n",
    "# A history log of obstacle (asteroids and nebulae) movement events.\n",
    "# - `True`: The ships' sensors detected a change in the obstacles' positions at this step.\n",
    "# - `False`: The sensors did not detect any changes.\n",
    "# This information will be used to determine the speed and direction of obstacle movement.\n",
    "OBSTACLES_MOVEMENT_STATUS = []\n",
    "\n",
    "# Others:\n",
    "\n",
    "# The energy on the unknown tiles will be used in the pathfinding\n",
    "HIDDEN_NODE_ENERGY = 0\n",
    "\n",
    "\n",
    "\n",
    "class NodeType(IntEnum):\n",
    "    unknown = -1\n",
    "    empty = 0\n",
    "    nebula = 1\n",
    "    asteroid = 2\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "_DIRECTIONS = [\n",
    "    (0, 0),  # center\n",
    "    (0, -1),  # up\n",
    "    (1, 0),  # right\n",
    "    (0, 1),  #  down\n",
    "    (-1, 0),  # left\n",
    "    (0, 0),  # sap\n",
    "]\n",
    "\n",
    "\n",
    "class ActionType(IntEnum):\n",
    "    center = 0\n",
    "    up = 1\n",
    "    right = 2\n",
    "    down = 3\n",
    "    left = 4\n",
    "    sap = 5\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    @classmethod\n",
    "    def from_coordinates(cls, current_position, next_position):\n",
    "        dx = next_position[0] - current_position[0]\n",
    "        dy = next_position[1] - current_position[1]\n",
    "\n",
    "        if dx < 0:\n",
    "            return ActionType.left\n",
    "        elif dx > 0:\n",
    "            return ActionType.right\n",
    "        elif dy < 0:\n",
    "            return ActionType.up\n",
    "        elif dy > 0:\n",
    "            return ActionType.down\n",
    "        else:\n",
    "            return ActionType.center\n",
    "\n",
    "    def to_direction(self):\n",
    "        return _DIRECTIONS[self]\n",
    "\n",
    "\n",
    "def get_match_step(step: int) -> int:\n",
    "    return step % (MAX_STEPS_IN_MATCH + 1)\n",
    "\n",
    "\n",
    "def get_match_number(step: int) -> int:\n",
    "    return step // (MAX_STEPS_IN_MATCH + 1)\n",
    "\n",
    "\n",
    "# def warp_int(x):\n",
    "#     if x >= SPACE_SIZE:\n",
    "#         x -= SPACE_SIZE\n",
    "#     elif x < 0:\n",
    "#         x += SPACE_SIZE\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def warp_point(x, y) -> tuple:\n",
    "#     return warp_int(x), warp_int(y)\n",
    "\n",
    "\n",
    "def get_opposite(x, y) -> tuple:\n",
    "    # Returns the mirrored point across the diagonal\n",
    "    return SPACE_SIZE - y - 1, SPACE_SIZE - x - 1\n",
    "\n",
    "\n",
    "def is_upper_sector(x, y) -> bool:\n",
    "    return SPACE_SIZE - x - 1 >= y\n",
    "\n",
    "\n",
    "def is_lower_sector(x, y) -> bool:\n",
    "    return SPACE_SIZE - x - 1 <= y\n",
    "\n",
    "\n",
    "def is_team_sector(team_id, x, y) -> bool:\n",
    "    return is_upper_sector(x, y) if team_id == 0 else is_lower_sector(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609b8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.utils import agent_selector\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "from gymnasium import spaces\n",
    "\n",
    "# modified from https://www.kaggle.com/code/yizhewang3/ppo-stable-baselines3\n",
    "\n",
    "def flatten_obs(base_obs, env_cfg):\n",
    "    \"\"\"\n",
    "    Convert the multi-agent observation dictionary for the *current* player\n",
    "    into the flattened dict structure you had.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sometimes there's an extra \"obs\" key; adapt as needed\n",
    "    if \"obs\" in base_obs:\n",
    "        base_obs = base_obs[\"obs\"]\n",
    "\n",
    "    flat_obs = {}\n",
    "        # 处理 units 数据\n",
    "    if \"units\" in base_obs:\n",
    "        flat_obs[\"units_position\"] = np.array(base_obs[\"units\"][\"position\"], dtype=np.int32)\n",
    "        flat_obs[\"units_energy\"] = np.array(base_obs[\"units\"][\"energy\"], dtype=np.int32)\n",
    "        # 如果 units_energy 的 shape 为 (NUM_TEAMS, MAX_UNITS) 则扩展一个维度\n",
    "        if flat_obs[\"units_energy\"].ndim == 2:\n",
    "            flat_obs[\"units_energy\"] = np.expand_dims(flat_obs[\"units_energy\"], axis=-1)\n",
    "    else:\n",
    "        flat_obs[\"units_position\"] = np.array(base_obs[\"units_position\"], dtype=np.int32)\n",
    "        flat_obs[\"units_energy\"] = np.array(base_obs[\"units_energy\"], dtype=np.int32)\n",
    "        if flat_obs[\"units_energy\"].ndim == 2:\n",
    "            flat_obs[\"units_energy\"] = np.expand_dims(flat_obs[\"units_energy\"], axis=-1)\n",
    "    \n",
    "    # 处理 units_mask\n",
    "    if \"units_mask\" in base_obs:\n",
    "        flat_obs[\"units_mask\"] = np.array(base_obs[\"units_mask\"], dtype=np.int8)\n",
    "    else:\n",
    "        flat_obs[\"units_mask\"] = np.zeros(flat_obs[\"units_position\"].shape[:2], dtype=np.int8)\n",
    "    \n",
    "    # 处理 sensor_mask：若返回的是 3D 数组，则取逻辑 or 得到全局 mask\n",
    "    sensor_mask_arr = np.array(base_obs[\"sensor_mask\"], dtype=np.int8)\n",
    "    if sensor_mask_arr.ndim == 3:\n",
    "        sensor_mask = np.any(sensor_mask_arr, axis=0).astype(np.int8)\n",
    "    else:\n",
    "        sensor_mask = sensor_mask_arr\n",
    "    flat_obs[\"sensor_mask\"] = sensor_mask\n",
    "\n",
    "    # 处理 map_features（tile_type 与 energy）\n",
    "    if \"map_features\" in base_obs:\n",
    "        mf = base_obs[\"map_features\"]\n",
    "        flat_obs[\"map_features_tile_type\"] = np.array(mf[\"tile_type\"], dtype=np.int8)\n",
    "        flat_obs[\"map_features_energy\"] = np.array(mf[\"energy\"], dtype=np.int8)\n",
    "    else:\n",
    "        flat_obs[\"map_features_tile_type\"] = np.array(base_obs[\"map_features_tile_type\"], dtype=np.int8)\n",
    "        flat_obs[\"map_features_energy\"] = np.array(base_obs[\"map_features_energy\"], dtype=np.int8)\n",
    "\n",
    "    # 处理 relic 节点信息\n",
    "    if \"relic_nodes_mask\" in base_obs:\n",
    "        flat_obs[\"relic_nodes_mask\"] = np.array(base_obs[\"relic_nodes_mask\"], dtype=np.int8)\n",
    "    else:\n",
    "        max_relic = env_cfg.get(\"max_relic_nodes\", 6) if env_cfg is not None else 6\n",
    "        flat_obs[\"relic_nodes_mask\"] = np.zeros((max_relic,), dtype=np.int8)\n",
    "    if \"relic_nodes\" in base_obs:\n",
    "        flat_obs[\"relic_nodes\"] = np.array(base_obs[\"relic_nodes\"], dtype=np.int32)\n",
    "    else:\n",
    "        max_relic = env_cfg.get(\"max_relic_nodes\", 6) if env_cfg is not None else 6\n",
    "        flat_obs[\"relic_nodes\"] = np.full((max_relic, 2), -1, dtype=np.int32)\n",
    "\n",
    "    # 处理团队得分与胜局\n",
    "    if \"team_points\" in base_obs:\n",
    "        flat_obs[\"team_points\"] = np.array(base_obs[\"team_points\"], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"team_points\"] = np.zeros(2, dtype=np.int32)\n",
    "    if \"team_wins\" in base_obs:\n",
    "        flat_obs[\"team_wins\"] = np.array(base_obs[\"team_wins\"], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"team_wins\"] = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "    # 处理步数信息\n",
    "    if \"steps\" in base_obs:\n",
    "        flat_obs[\"steps\"] = np.array([base_obs[\"steps\"]], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"steps\"] = np.array([0], dtype=np.int32)\n",
    "    if \"match_steps\" in base_obs:\n",
    "        flat_obs[\"match_steps\"] = np.array([base_obs[\"match_steps\"]], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"match_steps\"] = np.array([0], dtype=np.int32)\n",
    "\n",
    "    # 注意：不在此处处理 remainingOverageTime，\n",
    "    # 将在 Agent.act 中利用传入的参数添加\n",
    "\n",
    "    # 补全环境配置信息\n",
    "    if env_cfg is not None:\n",
    "        flat_obs[\"env_cfg_map_width\"] = np.array([env_cfg[\"map_width\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_map_height\"] = np.array([env_cfg[\"map_height\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_max_steps_in_match\"] = np.array([env_cfg[\"max_steps_in_match\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_move_cost\"] = np.array([env_cfg[\"unit_move_cost\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_cost\"] = np.array([env_cfg[\"unit_sap_cost\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_range\"] = np.array([env_cfg[\"unit_sap_range\"]], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"env_cfg_map_width\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_map_height\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_max_steps_in_match\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_move_cost\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_cost\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_range\"] = np.array([0], dtype=np.int32)\n",
    "\n",
    "    return flat_obs\n",
    "\n",
    "\n",
    "class LuxAICAECEnv(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"LuxAI-AEC\"}\n",
    "\n",
    "    def __init__(self, base_env):\n",
    "        \"\"\"\n",
    "        :param base_env: Your two-agent environment, e.g. LuxAIS3GymEnv, that returns:\n",
    "                         obs: {\"player_0\": {...}, \"player_1\": {...}}\n",
    "                         reward: {\"player_0\": float, \"player_1\": float}\n",
    "                         done/trunc: {\"player_0\": bool, \"player_1\": bool}, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_env = base_env\n",
    "\n",
    "        # In AEC, we specify the set of agents:\n",
    "        self.possible_agents = [\"player_0\", \"player_1\"]\n",
    "        self.agents = self.possible_agents[:]\n",
    "\n",
    "        # We'll manage done/trunc states in dictionaries\n",
    "        self.terminations = dict.fromkeys(self.agents, False)\n",
    "        self.truncations = dict.fromkeys(self.agents, False)\n",
    "        self.rewards = dict.fromkeys(self.agents, 0.0)\n",
    "        self.infos = dict.fromkeys(self.agents, {})\n",
    "        self._cumulative_rewards = dict.fromkeys(self.agents, 0.0)\n",
    "\n",
    "        # We store pending actions until we have both, if we want truly simultaneous stepping\n",
    "        # or we can do \"turn-by-turn\" in the sense that each step in the base env is a single move.\n",
    "        self._actions = {agent: None for agent in self.agents}\n",
    "\n",
    "        # Agent selector to cycle between player_0 -> player_1 -> player_0 ...\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "\n",
    "        # Construct observation_spaces and action_spaces for each agent\n",
    "        # from your flatten logic. The user code has a dictionary observation_space, so define that:\n",
    "        self.observation_spaces = {}\n",
    "        self.action_spaces = {}\n",
    "\n",
    "        # Example: replicate your dictionary-based observation space for each agent:\n",
    "        obs_space_dict = {\n",
    "            \"units_position\": spaces.Box(low=0, high=SPACE_SIZE - 1,\n",
    "                                         shape=(NUM_TEAMS, MAX_UNITS, 2), dtype=np.int32),\n",
    "            \"units_energy\": spaces.Box(low=0, high=400, shape=(NUM_TEAMS, MAX_UNITS, 1), dtype=np.int32),\n",
    "            \"units_mask\": spaces.Box(low=0, high=1, shape=(NUM_TEAMS, MAX_UNITS), dtype=np.int8),\n",
    "            \"sensor_mask\": spaces.Box(low=0, high=1, shape=(SPACE_SIZE, SPACE_SIZE), dtype=np.int8),\n",
    "            \"map_features_tile_type\": spaces.Box(low=-1, high=2, shape=(SPACE_SIZE, SPACE_SIZE), dtype=np.int8),\n",
    "            \"map_features_energy\": spaces.Box(low=-1, high=MAX_ENERGY_PER_TILE, shape=(SPACE_SIZE, SPACE_SIZE), dtype=np.int8),\n",
    "            \"relic_nodes_mask\": spaces.Box(low=0, high=1, shape=(MAX_RELIC_NODES,), dtype=np.int8),\n",
    "            \"relic_nodes\": spaces.Box(low=-1, high=SPACE_SIZE - 1, shape=(MAX_RELIC_NODES, 2), dtype=np.int32),\n",
    "            \"team_points\": spaces.Box(low=0, high=1000, shape=(NUM_TEAMS,), dtype=np.int32),\n",
    "            \"team_wins\": spaces.Box(low=0, high=1000, shape=(NUM_TEAMS,), dtype=np.int32),\n",
    "            \"steps\": spaces.Box(low=0, high=MAX_STEPS_IN_MATCH, shape=(1,), dtype=np.int32),\n",
    "            \"match_steps\": spaces.Box(low=0, high=MAX_STEPS_IN_MATCH, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_map_width\": spaces.Box(low=0, high=SPACE_SIZE, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_map_height\": spaces.Box(low=0, high=SPACE_SIZE, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_max_steps_in_match\": spaces.Box(low=0, high=MAX_STEPS_IN_MATCH, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_unit_move_cost\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_unit_sap_cost\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_unit_sap_range\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.int32),\n",
    "        }\n",
    "        # Create a Dict space from that\n",
    "        self.obs_space_single = spaces.Dict(obs_space_dict)\n",
    "\n",
    "        # Example action space: MultiDiscrete for each possible unit, as in your code\n",
    "        self.act_space_single = spaces.MultiDiscrete([len(ActionType)] * MAX_UNITS)\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.observation_spaces[agent] = self.obs_space_single\n",
    "            self.action_spaces[agent] = self.act_space_single\n",
    "\n",
    "        self.env_cfg = None\n",
    "        self.has_reset = False\n",
    "\n",
    "    def reset(self, seed=None, return_info=True, options=None):\n",
    "        obs_dict, info = self.base_env.reset(seed=seed, options=options)\n",
    "        self.env_cfg = info.get(\"params\", {})\n",
    "        self.has_reset = True\n",
    "        self.agents = self.possible_agents[:]\n",
    "        # Clear terminations/truncations\n",
    "        ret = {}\n",
    "        infos = {}\n",
    "        for ag in self.agents:\n",
    "            self.terminations[ag] = False\n",
    "            self.truncations[ag] = False\n",
    "            self.rewards[ag] = 0.0\n",
    "            self.infos[ag] = {}\n",
    "            self._cumulative_rewards[ag] = 0.0\n",
    "            ret[ag] = flatten_obs(obs_dict[ag], self.env_cfg)\n",
    "            infos[ag] = self.env_cfg\n",
    "\n",
    "        self._agent_selector.reset()\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "\n",
    "\n",
    "        return ret, infos\n",
    "\n",
    "    def step(self, simplified_actions_dict):\n",
    "        if not self.has_reset:\n",
    "            raise AssertionError(\"Environment must be reset before step().\")\n",
    "        action_dict = {}\n",
    "        for ag in self.agents:\n",
    "            actions = np.zeros((MAX_UNITS, 3), dtype=np.int16)\n",
    "\n",
    "            for i, action_type in enumerate(simplified_actions_dict[ag]):\n",
    "                dir_x, dir_y = _DIRECTIONS[action_type]\n",
    "                actions[i, 0] = action_type  # ActionType (0 to 5)\n",
    "                actions[i, 1] = dir_x        # delta_x\n",
    "                actions[i, 2] = dir_y        # delta_y\n",
    "            action_dict[ag] = np.array(actions, dtype=np.int16)\n",
    "        obs, rew, terminated_dict, truncated_dict, info = self.base_env.step(\n",
    "            action_dict\n",
    "        )\n",
    "        observations = {}\n",
    "        rewards = {}\n",
    "        terminations = {}\n",
    "        truncations = {}\n",
    "        infos = {}\n",
    "\n",
    "        # Some or all agents might be done. We keep them in self.agents until the end of the game \n",
    "        # if your logic calls for that. Or remove them once done. \n",
    "        # Typically in a 2-player zero-sum environment, if one agent is done => both are done.\n",
    "        # But you can do your own logic. For example:\n",
    "        new_active_agents = []\n",
    "\n",
    "        for ag in self.possible_agents:\n",
    "            observations[ag] = flatten_obs(obs[ag], self.env_cfg)\n",
    "            rewards[ag] = float(rew[ag])  # ensure it's float\n",
    "            terminations[ag] = bool(terminated_dict[ag])\n",
    "            truncations[ag] = bool(truncated_dict[ag])\n",
    "            infos[ag] = info.get(ag, {})\n",
    "\n",
    "            # If not done, we keep them in the list\n",
    "            if not (terminations[ag] or truncations[ag]):\n",
    "                new_active_agents.append(ag)\n",
    "\n",
    "        self.agents = new_active_agents\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        return self.base_env.render()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de398c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['units_position', 'units_energy', 'units_mask', 'sensor_mask', 'map_features_tile_type', 'map_features_energy', 'relic_nodes_mask', 'relic_nodes', 'team_points', 'team_wins', 'steps', 'match_steps', 'env_cfg_map_width', 'env_cfg_map_height', 'env_cfg_max_steps_in_match', 'env_cfg_unit_move_cost', 'env_cfg_unit_sap_cost', 'env_cfg_unit_sap_range'])\n"
     ]
    }
   ],
   "source": [
    "# from pettingzoo.utils import parallel_to_aec\n",
    "\n",
    "base_env = LuxAIS3GymEnv(numpy_output=True)\n",
    "wrapped_env = LuxAICAECEnv(base_env)\n",
    "obs, _ = wrapped_env.reset()\n",
    "print(obs['player_0'].keys())\n",
    "action_dict = {}\n",
    "for player, act_space in wrapped_env.action_spaces.items():\n",
    "    action_dict[player] = act_space.sample()\n",
    "obs, _, _, _, _ = wrapped_env.step(action_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "env = LuxAICAECEnv(LuxAIS3GymEnv(numpy_output=True))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        # We assume env.observation_spaces[\"player_0\"] and env.action_spaces[\"player_0\"] exist\n",
    "        self.net = MultiInputActorCriticPolicy(\n",
    "            observation_space=env.observation_spaces[\"player_0\"],\n",
    "            action_space=env.action_spaces[\"player_0\"],\n",
    "            lr_schedule=MultiInputActorCriticPolicy._dummy_schedule\n",
    "        )\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        features_vf = self.net.vf_features_extractor(obs).float()\n",
    "        latent_vf = self.net.mlp_extractor.value_net(features_vf)\n",
    "        value = self.net.value_net(latent_vf)\n",
    "        return value.squeeze(-1)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        features_pi = self.net.pi_features_extractor(obs).float()\n",
    "        latent_pi = self.net.mlp_extractor.policy_net(features_pi)\n",
    "        dist = self.net._get_action_dist_from_latent(latent_pi)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        if log_prob.ndim > 1:\n",
    "            # MultiDiscrete: sum log_probs across action dimensions\n",
    "            log_prob = log_prob.sum(-1)\n",
    "        entropy = dist.entropy()\n",
    "        if entropy.ndim > 1:\n",
    "            entropy = entropy.sum(-1)\n",
    "\n",
    "        # Compute value using separate extractor and head\n",
    "        features_vf = self.net.vf_features_extractor(obs).float()\n",
    "        latent_vf = self.net.mlp_extractor.value_net(features_vf)\n",
    "        value = self.net.value_net(latent_vf).squeeze(-1)\n",
    "        return action, log_prob, entropy, value\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Batchify/unbatchify for your dictionary-based obs\n",
    "# -------------------------------------------------------\n",
    "def batchify_obs(obs):\n",
    "    \"\"\"\n",
    "    obs is like:\n",
    "      {\n",
    "        'player_0': { 'units_position': ..., 'units_energy': ..., ... },\n",
    "        'player_1': { 'units_position': ..., 'units_energy': ..., ... }\n",
    "      }\n",
    "    Returns a dict of Tensors with shape [2, ...].\n",
    "    \"\"\"\n",
    "    batched_obs = {}\n",
    "    for key in obs['player_0']:\n",
    "        # For each key, convert each player's observation to a tensor and stack\n",
    "        stacked = []\n",
    "        for agent in ('player_0', 'player_1'):\n",
    "            stacked.append(torch.tensor(obs[agent][key], device=device))\n",
    "        batched_obs[key] = torch.stack(stacked, dim=0)\n",
    "    return batched_obs\n",
    "\n",
    "def unbatchify_actions(action_tensor):\n",
    "    \"\"\"\n",
    "    Converts a [2, ...] torch tensor of actions into:\n",
    "      { 'player_0': action_for_player_0, 'player_1': action_for_player_1 }\n",
    "    For a MultiDiscrete action of shape [16], each row is the 16-dim action.\n",
    "    \"\"\"\n",
    "    action_np = action_tensor.cpu().numpy()\n",
    "    return {\n",
    "        \"player_0\": action_np[0],\n",
    "        \"player_1\": action_np[1],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7488651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjayanth-b\u001b[0m (\u001b[33may2425s2-cs3263-group-13\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jayanth/Desktop/AY2425S2/CS3263/Lux-Design-S3/kits/python/wandb/run-20250414_184516-v5ytaw3y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ay2425s2-cs3263-group-13/lux-ppo/runs/v5ytaw3y' target=\"_blank\">exalted-snowball-1</a></strong> to <a href='https://wandb.ai/ay2425s2-cs3263-group-13/lux-ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ay2425s2-cs3263-group-13/lux-ppo' target=\"_blank\">https://wandb.ai/ay2425s2-cs3263-group-13/lux-ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ay2425s2-cs3263-group-13/lux-ppo/runs/v5ytaw3y' target=\"_blank\">https://wandb.ai/ay2425s2-cs3263-group-13/lux-ppo/runs/v5ytaw3y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/vzk3b09174b7wm89wghv48jm0000gn/T/ipykernel_52128/3456631176.py:71: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  next_done = torch.tensor([np.logical_or(trunc0, term0), np.logical_or(trunc1, term1)])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 238\u001b[39m\n\u001b[32m    234\u001b[39m     run.finish()\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLuxAICAECEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLuxAIS3GymEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_num_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(env, total_episodes, rollout_num_steps)\u001b[39m\n\u001b[32m    113\u001b[39m     next_val = rb_values[t + \u001b[32m1\u001b[39m]\n\u001b[32m    114\u001b[39m     done_mask = \u001b[32m1.0\u001b[39m - rb_dones[t + \u001b[32m1\u001b[39m].float()\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m delta = rb_rewards[t] + \u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_mask\u001b[49m - rb_values[t]\n\u001b[32m    117\u001b[39m gae = delta + gamma * gae_lambda * gae * done_mask\n\u001b[32m    118\u001b[39m rb_advantages[t] = gae\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# modified from https://pettingzoo.farama.org/tutorials/cleanrl/implementing_PPO/\n",
    "import wandb\n",
    "\n",
    "def train_ppo(env, total_episodes=2, rollout_num_steps=200):\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    train_epochs = 4\n",
    "    gae_lambda = 0.95\n",
    "    max_grad_norm = 0.5\n",
    "    lr = 2.5e-4\n",
    "    anneal_lr = True\n",
    "    seed = 2025\n",
    "    run = wandb.init(\n",
    "    entity=\"ay2425s2-cs3263-group-13\",\n",
    "    project=\"lux-ppo\",\n",
    "    config={\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"clip_coef\": clip_coef,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"train_epochs\": train_epochs,\n",
    "        \"total_episodes\": total_episodes,\n",
    "        \"max_steps_per_episode\": rollout_num_steps\n",
    "        },\n",
    "        save_code=True,\n",
    "    )\n",
    "    agent = Agent(env).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)\n",
    "\n",
    "    for episode in range(1, total_episodes + 1):\n",
    "        next_obs, info = env.reset(seed=seed)\n",
    "        total_episodic_return = np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        rb_obs = []\n",
    "        rb_actions = []\n",
    "        rb_logprobs = []\n",
    "        rb_rewards = []\n",
    "        rb_dones = []\n",
    "        rb_values = []\n",
    "\n",
    "        end_step = 0  # track how many steps actually took place\n",
    "         \n",
    "        if anneal_lr:\n",
    "            frac = 1.0 - (episode - 1.0) / total_episodes\n",
    "            lrnow = frac * lr\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        # 1. Collect experience\n",
    "        for step in range(rollout_num_steps):\n",
    "            obs_tensor = batchify_obs(next_obs)\n",
    "            with torch.no_grad():\n",
    "                actions, logprobs, entropy, values = agent.get_action_and_value(obs_tensor)\n",
    "\n",
    "            action_dict = unbatchify_actions(actions)\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(action_dict)\n",
    "\n",
    "            rb_obs.append(obs_tensor)\n",
    "            rb_actions.append(actions)\n",
    "            rb_logprobs.append(logprobs)\n",
    "            rb_values.append(values)\n",
    "            \n",
    "\n",
    "            r0, r1 = rewards[\"player_0\"], rewards[\"player_1\"]\n",
    "            trunc0, trunc1 = truncs[\"player_0\"], truncs[\"player_1\"]\n",
    "            term0, term1 = terms[\"player_0\"], terms[\"player_1\"]\n",
    "            next_done = torch.tensor([np.logical_or(trunc0, term0), np.logical_or(trunc1, term1)])\n",
    "            rb_rewards.append(torch.tensor([r0, r1], device=device))\n",
    "            rb_dones.append(next_done)\n",
    "\n",
    "            total_episodic_return += np.array([r0, r1])\n",
    "            end_step = step + 1\n",
    "\n",
    "            if all(terms.values()) or all(truncs.values()):\n",
    "                break\n",
    "\n",
    "        # 2. Bootstrapping if not done\n",
    "        with torch.no_grad():\n",
    "            if not all(terms.values()):\n",
    "                final_obs_tensor = batchify_obs(next_obs)\n",
    "                _, _, _, next_values = agent.get_action_and_value(final_obs_tensor)\n",
    "            else:\n",
    "                next_values = torch.zeros(2, device=device)\n",
    "\n",
    "        # 3. Convert lists -> stacked Tensors\n",
    "\n",
    "        num_steps = len(rb_obs)\n",
    "        stacked_obs = {}\n",
    "        for key in rb_obs[0].keys():\n",
    "            cat_list = [step_dict[key] for step_dict in rb_obs]\n",
    "            stacked_obs[key] = torch.stack(cat_list, dim=0)\n",
    "\n",
    "        rb_actions  = torch.stack(rb_actions, dim=0)   # [num_steps, 2, (action_dim)]\n",
    "        rb_logprobs = torch.stack(rb_logprobs, dim=0) # [num_steps, 2]\n",
    "        rb_values   = torch.stack(rb_values, dim=0)   # [num_steps, 2]\n",
    "        rb_rewards  = torch.stack(rb_rewards, dim=0)  # [num_steps, 2]\n",
    "        rb_dones    = torch.stack(rb_dones, dim=0)    # [num_steps, 2]\n",
    "\n",
    "        # 4. GAE or simple advantage\n",
    "        rb_advantages = torch.zeros_like(rb_rewards)\n",
    "        rb_returns = torch.zeros_like(rb_rewards)\n",
    "        gae = torch.zeros(2, device=device)\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                next_val = next_values\n",
    "                done_mask = 1.0 - rb_dones[t].float()\n",
    "            else:\n",
    "                next_val = rb_values[t + 1]\n",
    "                done_mask = 1.0 - rb_dones[t + 1].float()\n",
    "\n",
    "            delta = rb_rewards[t] + gamma * next_val * done_mask - rb_values[t]\n",
    "            gae = delta + gamma * gae_lambda * gae * done_mask\n",
    "            rb_advantages[t] = gae\n",
    "            rb_returns[t] = gae + rb_values[t]\n",
    "\n",
    "        # 5. Flatten time & agent\n",
    "        b_obs = {}\n",
    "        for key, val in stacked_obs.items():\n",
    "            b_obs[key] = val.view(num_steps * 2, *val.shape[2:])\n",
    "\n",
    "        b_actions    = rb_actions.view(num_steps * 2, -1)\n",
    "        b_logprobs   = rb_logprobs.view(num_steps * 2)\n",
    "        b_values     = rb_values.view(num_steps * 2)\n",
    "        b_advantages = rb_advantages.view(num_steps * 2)\n",
    "        b_returns    = rb_returns.view(num_steps * 2)\n",
    "\n",
    "        # We'll track these for logging outside the minibatch loop\n",
    "        clip_fracs = []\n",
    "        old_approx_kls = []\n",
    "        approx_kls = []\n",
    "        last_v_loss = 0.0\n",
    "        last_pg_loss = 0.0\n",
    "\n",
    "        # 6. PPO update\n",
    "        total_batch = num_steps * 2\n",
    "        indices = np.arange(total_batch)\n",
    "        for _ in range(train_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, total_batch, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_inds = indices[start:end]\n",
    "\n",
    "                mb_obs = {k: v[batch_inds] for k, v in b_obs.items()}\n",
    "                mb_actions = b_actions[batch_inds]\n",
    "                mb_old_logprob = b_logprobs[batch_inds]\n",
    "                mb_adv = b_advantages[batch_inds]\n",
    "                mb_returns = b_returns[batch_inds]\n",
    "                mb_values = b_values[batch_inds]\n",
    "                 # Evaluate new logprob\n",
    "                _, new_logprob, entropy, value = agent.get_action_and_value(\n",
    "                    mb_obs, action=mb_actions\n",
    "                )\n",
    "                logratio = new_logprob - mb_old_logprob\n",
    "                ratio = logratio.exp()\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "                # Record them\n",
    "                old_approx_kls.append(old_approx_kl.item())\n",
    "                approx_kls.append(approx_kl.item())\n",
    "\n",
    "                # Compute clip fraction\n",
    "                clip_fraction = ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                clip_fracs.append(clip_fraction)\n",
    "\n",
    "                # Normalize advantages\n",
    "                mb_adv = (mb_adv - mb_adv.mean()) / (mb_adv.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_adv * ratio\n",
    "                pg_loss2 = -mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss (clipped)\n",
    "                value = value.view(-1)\n",
    "                v_loss_unclipped = (value - mb_returns) ** 2\n",
    "                v_clipped = mb_values + torch.clamp(value - mb_values, -clip_coef, clip_coef)\n",
    "                v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
    "                v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
    "\n",
    "                # Entropy\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss + vf_coef * v_loss - ent_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                # We'll keep track of the last minibatch losses for logging\n",
    "                last_v_loss = v_loss.item()\n",
    "                last_pg_loss = pg_loss.item()\n",
    "\n",
    "        # 7. Explained Variance\n",
    "        y_pred = b_values.detach().cpu().numpy()\n",
    "        y_true = b_returns.detach().cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        run.log({\n",
    "            \"episode\": episode,\n",
    "            \"episode_length\": end_step,\n",
    "            \"episodic_return\": np.mean(total_episodic_return),\n",
    "            \"policy_loss\": last_pg_loss,\n",
    "            \"value_loss\": last_v_loss,\n",
    "            \"old_approx_kl\": np.mean(old_approx_kls),\n",
    "            \"approx_kl\": np.mean(approx_kls),\n",
    "            \"clip_fraction\": np.mean(clip_fracs),\n",
    "            \"explained_variance\": explained_var\n",
    "        })\n",
    "\n",
    "        # 8. Logging (similar structure to your snippet)\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {np.mean(total_episodic_return[0])}\") # zero sum game, so log only 0th agent returns\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {last_v_loss}\")\n",
    "        print(f\"Policy Loss: {last_pg_loss}\")\n",
    "        print(f\"Old Approx KL: {np.mean(old_approx_kls)}\")\n",
    "        print(f\"Approx KL: {np.mean(approx_kls)}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    torch.save(agent.state_dict(), f\"./checkpoints/model_ep{episode}.pt\")\n",
    "    # Upload to wandb\n",
    "    wandb.save(f\"model_ep{episode}.pt\")\n",
    "    run.finish()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "train_ppo(LuxAICAECEnv(LuxAIS3GymEnv(numpy_output=True)), total_episodes=1000, rollout_num_steps=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
