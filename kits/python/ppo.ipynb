{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1158d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from enum import IntEnum\n",
    "\n",
    "# taken from https://www.kaggle.com/code/yizhewang3/ppo-stable-baselines3\n",
    "\n",
    "SPACE_SIZE = 24\n",
    "NUM_TEAMS = 2\n",
    "MAX_UNITS = 16\n",
    "RELIC_REWARD_RANGE = 2\n",
    "MAX_STEPS_IN_MATCH = 100\n",
    "MAX_ENERGY_PER_TILE = 20\n",
    "MAX_RELIC_NODES = 6\n",
    "LAST_MATCH_STEP_WHEN_RELIC_CAN_APPEAR = 50\n",
    "LAST_MATCH_WHEN_RELIC_CAN_APPEAR = 2\n",
    "\n",
    "# We will find the exact value of these constants during the game\n",
    "UNIT_MOVE_COST = 1  # OPTIONS: list(range(1, 6))\n",
    "UNIT_SAP_COST = 30  # OPTIONS: list(range(30, 51))\n",
    "UNIT_SAP_RANGE = 3  # OPTIONS: list(range(3, 8))\n",
    "UNIT_SENSOR_RANGE = 2  # OPTIONS: [1, 2, 3, 4]\n",
    "OBSTACLE_MOVEMENT_PERIOD = 20  # OPTIONS: 6.67, 10, 20, 40\n",
    "OBSTACLE_MOVEMENT_DIRECTION = (0, 0)  # OPTIONS: [(1, -1), (-1, 1)]\n",
    "\n",
    "# We will NOT find the exact value of these constants during the game\n",
    "NEBULA_ENERGY_REDUCTION = 5  # OPTIONS: [0, 1, 2, 3, 5, 25]\n",
    "\n",
    "# Exploration flags:\n",
    "\n",
    "ALL_RELICS_FOUND = False\n",
    "ALL_REWARDS_FOUND = False\n",
    "OBSTACLE_MOVEMENT_PERIOD_FOUND = False\n",
    "OBSTACLE_MOVEMENT_DIRECTION_FOUND = False\n",
    "\n",
    "# Game logs:\n",
    "\n",
    "# REWARD_RESULTS: [{\"nodes\": Set[Node], \"points\": int}, ...]\n",
    "# A history of reward events, where each entry contains:\n",
    "# - \"nodes\": A set of nodes where our ships were located.\n",
    "# - \"points\": The number of points scored at that location.\n",
    "# This data will help identify which nodes yield points.\n",
    "REWARD_RESULTS = []\n",
    "\n",
    "# obstacles_movement_status: list of bool\n",
    "# A history log of obstacle (asteroids and nebulae) movement events.\n",
    "# - `True`: The ships' sensors detected a change in the obstacles' positions at this step.\n",
    "# - `False`: The sensors did not detect any changes.\n",
    "# This information will be used to determine the speed and direction of obstacle movement.\n",
    "OBSTACLES_MOVEMENT_STATUS = []\n",
    "\n",
    "# Others:\n",
    "\n",
    "# The energy on the unknown tiles will be used in the pathfinding\n",
    "HIDDEN_NODE_ENERGY = 0\n",
    "\n",
    "\n",
    "\n",
    "class NodeType(IntEnum):\n",
    "    unknown = -1\n",
    "    empty = 0\n",
    "    nebula = 1\n",
    "    asteroid = 2\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "_DIRECTIONS = [\n",
    "    (0, 0),  # center\n",
    "    (0, -1),  # up\n",
    "    (1, 0),  # right\n",
    "    (0, 1),  #  down\n",
    "    (-1, 0),  # left\n",
    "    (0, 0),  # sap\n",
    "]\n",
    "\n",
    "\n",
    "class ActionType(IntEnum):\n",
    "    center = 0\n",
    "    up = 1\n",
    "    right = 2\n",
    "    down = 3\n",
    "    left = 4\n",
    "    sap = 5\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    @classmethod\n",
    "    def from_coordinates(cls, current_position, next_position):\n",
    "        dx = next_position[0] - current_position[0]\n",
    "        dy = next_position[1] - current_position[1]\n",
    "\n",
    "        if dx < 0:\n",
    "            return ActionType.left\n",
    "        elif dx > 0:\n",
    "            return ActionType.right\n",
    "        elif dy < 0:\n",
    "            return ActionType.up\n",
    "        elif dy > 0:\n",
    "            return ActionType.down\n",
    "        else:\n",
    "            return ActionType.center\n",
    "\n",
    "    def to_direction(self):\n",
    "        return _DIRECTIONS[self]\n",
    "\n",
    "\n",
    "def get_match_step(step: int) -> int:\n",
    "    return step % (MAX_STEPS_IN_MATCH + 1)\n",
    "\n",
    "\n",
    "def get_match_number(step: int) -> int:\n",
    "    return step // (MAX_STEPS_IN_MATCH + 1)\n",
    "\n",
    "\n",
    "# def warp_int(x):\n",
    "#     if x >= SPACE_SIZE:\n",
    "#         x -= SPACE_SIZE\n",
    "#     elif x < 0:\n",
    "#         x += SPACE_SIZE\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def warp_point(x, y) -> tuple:\n",
    "#     return warp_int(x), warp_int(y)\n",
    "\n",
    "\n",
    "def get_opposite(x, y) -> tuple:\n",
    "    # Returns the mirrored point across the diagonal\n",
    "    return SPACE_SIZE - y - 1, SPACE_SIZE - x - 1\n",
    "\n",
    "\n",
    "def is_upper_sector(x, y) -> bool:\n",
    "    return SPACE_SIZE - x - 1 >= y\n",
    "\n",
    "\n",
    "def is_lower_sector(x, y) -> bool:\n",
    "    return SPACE_SIZE - x - 1 <= y\n",
    "\n",
    "\n",
    "def is_team_sector(team_id, x, y) -> bool:\n",
    "    return is_upper_sector(x, y) if team_id == 0 else is_lower_sector(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609b8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.utils import agent_selector\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "from gymnasium import spaces\n",
    "\n",
    "# modified from https://www.kaggle.com/code/yizhewang3/ppo-stable-baselines3\n",
    "\n",
    "def flatten_obs(base_obs, env_cfg):\n",
    "    \"\"\"\n",
    "    Convert the multi-agent observation dictionary for the *current* player\n",
    "    into the flattened dict structure you had.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sometimes there's an extra \"obs\" key; adapt as needed\n",
    "    if \"obs\" in base_obs:\n",
    "        base_obs = base_obs[\"obs\"]\n",
    "\n",
    "    flat_obs = {}\n",
    "        # 处理 units 数据\n",
    "    if \"units\" in base_obs:\n",
    "        flat_obs[\"units_position\"] = np.array(base_obs[\"units\"][\"position\"], dtype=np.int32)\n",
    "        flat_obs[\"units_energy\"] = np.array(base_obs[\"units\"][\"energy\"], dtype=np.int32)\n",
    "        # 如果 units_energy 的 shape 为 (NUM_TEAMS, MAX_UNITS) 则扩展一个维度\n",
    "        if flat_obs[\"units_energy\"].ndim == 2:\n",
    "            flat_obs[\"units_energy\"] = np.expand_dims(flat_obs[\"units_energy\"], axis=-1)\n",
    "    else:\n",
    "        flat_obs[\"units_position\"] = np.array(base_obs[\"units_position\"], dtype=np.int32)\n",
    "        flat_obs[\"units_energy\"] = np.array(base_obs[\"units_energy\"], dtype=np.int32)\n",
    "        if flat_obs[\"units_energy\"].ndim == 2:\n",
    "            flat_obs[\"units_energy\"] = np.expand_dims(flat_obs[\"units_energy\"], axis=-1)\n",
    "    \n",
    "    # 处理 units_mask\n",
    "    if \"units_mask\" in base_obs:\n",
    "        flat_obs[\"units_mask\"] = np.array(base_obs[\"units_mask\"], dtype=np.int8)\n",
    "    else:\n",
    "        flat_obs[\"units_mask\"] = np.zeros(flat_obs[\"units_position\"].shape[:2], dtype=np.int8)\n",
    "    \n",
    "    # 处理 sensor_mask：若返回的是 3D 数组，则取逻辑 or 得到全局 mask\n",
    "    sensor_mask_arr = np.array(base_obs[\"sensor_mask\"], dtype=np.int8)\n",
    "    if sensor_mask_arr.ndim == 3:\n",
    "        sensor_mask = np.any(sensor_mask_arr, axis=0).astype(np.int8)\n",
    "    else:\n",
    "        sensor_mask = sensor_mask_arr\n",
    "    flat_obs[\"sensor_mask\"] = sensor_mask\n",
    "\n",
    "    # 处理 map_features（tile_type 与 energy）\n",
    "    if \"map_features\" in base_obs:\n",
    "        mf = base_obs[\"map_features\"]\n",
    "        flat_obs[\"map_features_tile_type\"] = np.array(mf[\"tile_type\"], dtype=np.int8)\n",
    "        flat_obs[\"map_features_energy\"] = np.array(mf[\"energy\"], dtype=np.int8)\n",
    "    else:\n",
    "        flat_obs[\"map_features_tile_type\"] = np.array(base_obs[\"map_features_tile_type\"], dtype=np.int8)\n",
    "        flat_obs[\"map_features_energy\"] = np.array(base_obs[\"map_features_energy\"], dtype=np.int8)\n",
    "\n",
    "    # 处理 relic 节点信息\n",
    "    if \"relic_nodes_mask\" in base_obs:\n",
    "        flat_obs[\"relic_nodes_mask\"] = np.array(base_obs[\"relic_nodes_mask\"], dtype=np.int8)\n",
    "    else:\n",
    "        max_relic = env_cfg.get(\"max_relic_nodes\", 6) if env_cfg is not None else 6\n",
    "        flat_obs[\"relic_nodes_mask\"] = np.zeros((max_relic,), dtype=np.int8)\n",
    "    if \"relic_nodes\" in base_obs:\n",
    "        flat_obs[\"relic_nodes\"] = np.array(base_obs[\"relic_nodes\"], dtype=np.int32)\n",
    "    else:\n",
    "        max_relic = env_cfg.get(\"max_relic_nodes\", 6) if env_cfg is not None else 6\n",
    "        flat_obs[\"relic_nodes\"] = np.full((max_relic, 2), -1, dtype=np.int32)\n",
    "\n",
    "    # 处理团队得分与胜局\n",
    "    if \"team_points\" in base_obs:\n",
    "        flat_obs[\"team_points\"] = np.array(base_obs[\"team_points\"], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"team_points\"] = np.zeros(2, dtype=np.int32)\n",
    "    if \"team_wins\" in base_obs:\n",
    "        flat_obs[\"team_wins\"] = np.array(base_obs[\"team_wins\"], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"team_wins\"] = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "    # 处理步数信息\n",
    "    if \"steps\" in base_obs:\n",
    "        flat_obs[\"steps\"] = np.array([base_obs[\"steps\"]], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"steps\"] = np.array([0], dtype=np.int32)\n",
    "    if \"match_steps\" in base_obs:\n",
    "        flat_obs[\"match_steps\"] = np.array([base_obs[\"match_steps\"]], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"match_steps\"] = np.array([0], dtype=np.int32)\n",
    "\n",
    "    # 注意：不在此处处理 remainingOverageTime，\n",
    "    # 将在 Agent.act 中利用传入的参数添加\n",
    "\n",
    "    # 补全环境配置信息\n",
    "    if env_cfg is not None:\n",
    "        flat_obs[\"env_cfg_map_width\"] = np.array([env_cfg[\"map_width\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_map_height\"] = np.array([env_cfg[\"map_height\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_max_steps_in_match\"] = np.array([env_cfg[\"max_steps_in_match\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_move_cost\"] = np.array([env_cfg[\"unit_move_cost\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_cost\"] = np.array([env_cfg[\"unit_sap_cost\"]], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_range\"] = np.array([env_cfg[\"unit_sap_range\"]], dtype=np.int32)\n",
    "    else:\n",
    "        flat_obs[\"env_cfg_map_width\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_map_height\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_max_steps_in_match\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_move_cost\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_cost\"] = np.array([0], dtype=np.int32)\n",
    "        flat_obs[\"env_cfg_unit_sap_range\"] = np.array([0], dtype=np.int32)\n",
    "\n",
    "    return flat_obs\n",
    "\n",
    "\n",
    "class LuxAICAECEnv(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"LuxAI-AEC\"}\n",
    "\n",
    "    def __init__(self, base_env):\n",
    "        \"\"\"\n",
    "        :param base_env: Your two-agent environment, e.g. LuxAIS3GymEnv, that returns:\n",
    "                         obs: {\"player_0\": {...}, \"player_1\": {...}}\n",
    "                         reward: {\"player_0\": float, \"player_1\": float}\n",
    "                         done/trunc: {\"player_0\": bool, \"player_1\": bool}, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_env = base_env\n",
    "\n",
    "        # In AEC, we specify the set of agents:\n",
    "        self.possible_agents = [\"player_0\", \"player_1\"]\n",
    "        self.agents = self.possible_agents[:]\n",
    "\n",
    "        # We'll manage done/trunc states in dictionaries\n",
    "        self.terminations = dict.fromkeys(self.agents, False)\n",
    "        self.truncations = dict.fromkeys(self.agents, False)\n",
    "        self.rewards = dict.fromkeys(self.agents, 0.0)\n",
    "        self.infos = dict.fromkeys(self.agents, {})\n",
    "        self._cumulative_rewards = dict.fromkeys(self.agents, 0.0)\n",
    "\n",
    "        # We store pending actions until we have both, if we want truly simultaneous stepping\n",
    "        # or we can do \"turn-by-turn\" in the sense that each step in the base env is a single move.\n",
    "        self._actions = {agent: None for agent in self.agents}\n",
    "\n",
    "        # Agent selector to cycle between player_0 -> player_1 -> player_0 ...\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "\n",
    "        # Construct observation_spaces and action_spaces for each agent\n",
    "        # from your flatten logic. The user code has a dictionary observation_space, so define that:\n",
    "        self.observation_spaces = {}\n",
    "        self.action_spaces = {}\n",
    "\n",
    "        # Example: replicate your dictionary-based observation space for each agent:\n",
    "        obs_space_dict = {\n",
    "            \"units_position\": spaces.Box(low=0, high=SPACE_SIZE - 1,\n",
    "                                         shape=(NUM_TEAMS, MAX_UNITS, 2), dtype=np.int32),\n",
    "            \"units_energy\": spaces.Box(low=0, high=400, shape=(NUM_TEAMS, MAX_UNITS, 1), dtype=np.int32),\n",
    "            \"units_mask\": spaces.Box(low=0, high=1, shape=(NUM_TEAMS, MAX_UNITS), dtype=np.int8),\n",
    "            \"sensor_mask\": spaces.Box(low=0, high=1, shape=(SPACE_SIZE, SPACE_SIZE), dtype=np.int8),\n",
    "            \"map_features_tile_type\": spaces.Box(low=-1, high=2, shape=(SPACE_SIZE, SPACE_SIZE), dtype=np.int8),\n",
    "            \"map_features_energy\": spaces.Box(low=-1, high=MAX_ENERGY_PER_TILE, shape=(SPACE_SIZE, SPACE_SIZE), dtype=np.int8),\n",
    "            \"relic_nodes_mask\": spaces.Box(low=0, high=1, shape=(MAX_RELIC_NODES,), dtype=np.int8),\n",
    "            \"relic_nodes\": spaces.Box(low=-1, high=SPACE_SIZE - 1, shape=(MAX_RELIC_NODES, 2), dtype=np.int32),\n",
    "            \"team_points\": spaces.Box(low=0, high=1000, shape=(NUM_TEAMS,), dtype=np.int32),\n",
    "            \"team_wins\": spaces.Box(low=0, high=1000, shape=(NUM_TEAMS,), dtype=np.int32),\n",
    "            \"steps\": spaces.Box(low=0, high=MAX_STEPS_IN_MATCH, shape=(1,), dtype=np.int32),\n",
    "            \"match_steps\": spaces.Box(low=0, high=MAX_STEPS_IN_MATCH, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_map_width\": spaces.Box(low=0, high=SPACE_SIZE, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_map_height\": spaces.Box(low=0, high=SPACE_SIZE, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_max_steps_in_match\": spaces.Box(low=0, high=MAX_STEPS_IN_MATCH, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_unit_move_cost\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_unit_sap_cost\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.int32),\n",
    "            \"env_cfg_unit_sap_range\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.int32),\n",
    "        }\n",
    "        # Create a Dict space from that\n",
    "        self.obs_space_single = spaces.Dict(obs_space_dict)\n",
    "\n",
    "        # Example action space: MultiDiscrete for each possible unit, as in your code\n",
    "        self.act_space_single = spaces.MultiDiscrete([len(ActionType)] * MAX_UNITS)\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.observation_spaces[agent] = self.obs_space_single\n",
    "            self.action_spaces[agent] = self.act_space_single\n",
    "\n",
    "        self.env_cfg = None\n",
    "        self.has_reset = False\n",
    "\n",
    "    def reset(self, seed=None, return_info=True, options=None):\n",
    "        obs_dict, info = self.base_env.reset(seed=seed, options=options)\n",
    "        self.env_cfg = info.get(\"params\", {})\n",
    "        self.has_reset = True\n",
    "        self.agents = self.possible_agents[:]\n",
    "        # Clear terminations/truncations\n",
    "        ret = {}\n",
    "        infos = {}\n",
    "        for ag in self.agents:\n",
    "            self.terminations[ag] = False\n",
    "            self.truncations[ag] = False\n",
    "            self.rewards[ag] = 0.0\n",
    "            self.infos[ag] = {}\n",
    "            self._cumulative_rewards[ag] = 0.0\n",
    "            ret[ag] = flatten_obs(obs_dict[ag], self.env_cfg)\n",
    "            infos[ag] = self.env_cfg\n",
    "\n",
    "        self._agent_selector.reset()\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "\n",
    "\n",
    "        return ret, infos\n",
    "\n",
    "    def step(self, simplified_actions_dict):\n",
    "        if not self.has_reset:\n",
    "            raise AssertionError(\"Environment must be reset before step().\")\n",
    "        action_dict = {}\n",
    "        for ag in self.agents:\n",
    "            actions = np.zeros((MAX_UNITS, 3), dtype=np.int16)\n",
    "\n",
    "            for i, action_type in enumerate(simplified_actions_dict[ag]):\n",
    "                dir_x, dir_y = _DIRECTIONS[action_type]\n",
    "                actions[i, 0] = action_type  # ActionType (0 to 5)\n",
    "                actions[i, 1] = dir_x        # delta_x\n",
    "                actions[i, 2] = dir_y        # delta_y\n",
    "            action_dict[ag] = np.array(actions, dtype=np.int16)\n",
    "        obs, rew, terminated_dict, truncated_dict, info = self.base_env.step(\n",
    "            action_dict\n",
    "        )\n",
    "        observations = {}\n",
    "        rewards = {}\n",
    "        terminations = {}\n",
    "        truncations = {}\n",
    "        infos = {}\n",
    "\n",
    "        # Some or all agents might be done. We keep them in self.agents until the end of the game \n",
    "        # if your logic calls for that. Or remove them once done. \n",
    "        # Typically in a 2-player zero-sum environment, if one agent is done => both are done.\n",
    "        # But you can do your own logic. For example:\n",
    "        new_active_agents = []\n",
    "\n",
    "        for ag in self.possible_agents:\n",
    "            observations[ag] = flatten_obs(obs[ag], self.env_cfg)\n",
    "            rewards[ag] = float(rew[ag])  # ensure it's float\n",
    "            terminations[ag] = bool(terminated_dict[ag])\n",
    "            truncations[ag] = bool(truncated_dict[ag])\n",
    "            infos[ag] = info.get(ag, {})\n",
    "\n",
    "            # If not done, we keep them in the list\n",
    "            if not (terminations[ag] or truncations[ag]):\n",
    "                new_active_agents.append(ag)\n",
    "\n",
    "        self.agents = new_active_agents\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        return self.base_env.render()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de398c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['units_position', 'units_energy', 'units_mask', 'sensor_mask', 'map_features_tile_type', 'map_features_energy', 'relic_nodes_mask', 'relic_nodes', 'team_points', 'team_wins', 'steps', 'match_steps', 'env_cfg_map_width', 'env_cfg_map_height', 'env_cfg_max_steps_in_match', 'env_cfg_unit_move_cost', 'env_cfg_unit_sap_cost', 'env_cfg_unit_sap_range'])\n"
     ]
    }
   ],
   "source": [
    "# from pettingzoo.utils import parallel_to_aec\n",
    "\n",
    "base_env = LuxAIS3GymEnv(numpy_output=True)\n",
    "wrapped_env = LuxAICAECEnv(base_env)\n",
    "obs, _ = wrapped_env.reset()\n",
    "print(obs['player_0'].keys())\n",
    "action_dict = {}\n",
    "for player, act_space in wrapped_env.action_spaces.items():\n",
    "    action_dict[player] = act_space.sample()\n",
    "obs, _, _, _, _ = wrapped_env.step(action_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907fb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "env = LuxAICAECEnv(LuxAIS3GymEnv(numpy_output=True))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        # We assume env.observation_spaces[\"player_0\"] and env.action_spaces[\"player_0\"] exist\n",
    "        self.net = MultiInputActorCriticPolicy(\n",
    "            observation_space=env.observation_spaces[\"player_0\"],\n",
    "            action_space=env.action_spaces[\"player_0\"],\n",
    "            lr_schedule=MultiInputActorCriticPolicy._dummy_schedule\n",
    "        )\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        features_vf = self.net.vf_features_extractor(obs).float()\n",
    "        latent_vf = self.net.mlp_extractor.value_net(features_vf)\n",
    "        value = self.net.value_net(latent_vf)\n",
    "        return value.squeeze(-1)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        features_pi = self.net.pi_features_extractor(obs).float()\n",
    "        latent_pi = self.net.mlp_extractor.policy_net(features_pi)\n",
    "        dist = self.net._get_action_dist_from_latent(latent_pi)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        if log_prob.ndim > 1:\n",
    "            # MultiDiscrete: sum log_probs across action dimensions\n",
    "            log_prob = log_prob.sum(-1)\n",
    "        entropy = dist.entropy()\n",
    "        if entropy.ndim > 1:\n",
    "            entropy = entropy.sum(-1)\n",
    "\n",
    "        # Compute value using separate extractor and head\n",
    "        features_vf = self.net.vf_features_extractor(obs).float()\n",
    "        latent_vf = self.net.mlp_extractor.value_net(features_vf)\n",
    "        value = self.net.value_net(latent_vf).squeeze(-1)\n",
    "        return action, log_prob, entropy, value\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Batchify/unbatchify for your dictionary-based obs\n",
    "# -------------------------------------------------------\n",
    "def batchify_obs(obs):\n",
    "    \"\"\"\n",
    "    obs is like:\n",
    "      {\n",
    "        'player_0': { 'units_position': ..., 'units_energy': ..., ... },\n",
    "        'player_1': { 'units_position': ..., 'units_energy': ..., ... }\n",
    "      }\n",
    "    Returns a dict of Tensors with shape [2, ...].\n",
    "    \"\"\"\n",
    "    batched_obs = {}\n",
    "    for key in obs['player_0']:\n",
    "        # For each key, convert each player's observation to a tensor and stack\n",
    "        stacked = []\n",
    "        for agent in ('player_0', 'player_1'):\n",
    "            stacked.append(torch.tensor(obs[agent][key], device=device))\n",
    "        batched_obs[key] = torch.stack(stacked, dim=0)\n",
    "    return batched_obs\n",
    "\n",
    "def unbatchify_actions(action_tensor):\n",
    "    \"\"\"\n",
    "    Converts a [2, ...] torch tensor of actions into:\n",
    "      { 'player_0': action_for_player_0, 'player_1': action_for_player_1 }\n",
    "    For a MultiDiscrete action of shape [16], each row is the 16-dim action.\n",
    "    \"\"\"\n",
    "    action_np = action_tensor.cpu().numpy()\n",
    "    return {\n",
    "        \"player_0\": action_np[0],\n",
    "        \"player_1\": action_np[1],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7488651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjayanth-b\u001b[0m (\u001b[33may2425s2-cs3263-group-13\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 239\u001b[39m\n\u001b[32m    235\u001b[39m     run.finish()\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLuxAICAECEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLuxAIS3GymEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_num_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(env, total_episodes, rollout_num_steps)\u001b[39m\n\u001b[32m     14\u001b[39m anneal_lr = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     15\u001b[39m seed = \u001b[32m2025\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m run = \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43may2425s2-cs3263-group-13\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlux-ppo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment_coef\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvf_coef\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclip_coef\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal_episodes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_steps_per_episode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_num_steps\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m agent = Agent(env).to(device)\n\u001b[32m     33\u001b[39m optimizer = optim.Adam(agent.parameters(), lr=lr, eps=\u001b[32m1e-5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:1530\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[39m\n\u001b[32m   1527\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_settings.x_server_side_derived_summary:\n\u001b[32m   1528\u001b[39m             init_telemetry.feature.server_side_derived_summary = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1533\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wl:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:965\u001b[39m, in \u001b[36m_WandbInit.init\u001b[39m\u001b[34m(self, settings, config)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    964\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m         result = \u001b[43mwait_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_init_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_init_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m    973\u001b[39m         run_init_handle.cancel(backend.interface)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py:24\u001b[39m, in \u001b[36mwait_with_progress\u001b[39m\u001b[34m(handle, timeout, progress_after, display_progress)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_with_progress\u001b[39m(\n\u001b[32m     14\u001b[39m     handle: MailboxHandle[_T],\n\u001b[32m     15\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     display_progress: Callable[[], Coroutine[Any, Any, \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[32m     19\u001b[39m ) -> _T:\n\u001b[32m     20\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wait for a handle, possibly displaying progress to the user.\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[33;03m    Equivalent to passing a single handle to `wait_all_with_progress`.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_all_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py:70\u001b[39m, in \u001b[36mwait_all_with_progress\u001b[39m\u001b[34m(handle_list, timeout, progress_after, display_progress)\u001b[39m\n\u001b[32m     67\u001b[39m start_time = time.monotonic()\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wait_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py:110\u001b[39m, in \u001b[36m_wait_handles\u001b[39m\u001b[34m(handle_list, timeout)\u001b[39m\n\u001b[32m    108\u001b[39m     elapsed_time = time.monotonic() - start_time\n\u001b[32m    109\u001b[39m     remaining_timeout = timeout - elapsed_time\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     results.append(\u001b[43mhandle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_timeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/mailbox/mailbox_handle.py:122\u001b[39m, in \u001b[36m_MailboxMappedHandle.wait_or\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_or\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, timeout: \u001b[38;5;28mfloat\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m) -> _S:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AY2425S2/CS3263/Lux-Design-S3/.venv/lib/python3.12/site-packages/wandb/sdk/mailbox/response_handle.py:88\u001b[39m, in \u001b[36mMailboxResponseHandle.wait_or\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math.isfinite(timeout):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout must be finite or None.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m     90\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimed out waiting for response on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._address\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# modified from https://pettingzoo.farama.org/tutorials/cleanrl/implementing_PPO/\n",
    "import wandb\n",
    "\n",
    "def train_ppo(env, total_episodes=2, rollout_num_steps=200):\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    train_epochs = 4\n",
    "    gae_lambda = 0.95\n",
    "    max_grad_norm = 0.5\n",
    "    lr = 2.5e-4\n",
    "    anneal_lr = True\n",
    "    seed = 2025\n",
    "    run = wandb.init(\n",
    "    entity=\"ay2425s2-cs3263-group-13\",\n",
    "    project=\"lux-ppo\",\n",
    "    config={\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"clip_coef\": clip_coef,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"train_epochs\": train_epochs,\n",
    "        \"total_episodes\": total_episodes,\n",
    "        \"max_steps_per_episode\": rollout_num_steps\n",
    "        },\n",
    "        save_code=True,\n",
    "    )\n",
    "    agent = Agent(env).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)\n",
    "\n",
    "    for episode in range(1, total_episodes + 1):\n",
    "        next_obs, info = env.reset(seed=seed)\n",
    "        total_episodic_return = np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        rb_obs = []\n",
    "        rb_actions = []\n",
    "        rb_logprobs = []\n",
    "        rb_rewards = []\n",
    "        rb_dones = []\n",
    "        rb_values = []\n",
    "\n",
    "        end_step = 0  # track how many steps actually took place\n",
    "         \n",
    "        if anneal_lr:\n",
    "            frac = 1.0 - (episode - 1.0) / total_episodes\n",
    "            lrnow = frac * lr\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        # 1. Collect experience\n",
    "        for step in range(rollout_num_steps):\n",
    "            obs_tensor = batchify_obs(next_obs)\n",
    "            with torch.no_grad():\n",
    "                actions, logprobs, entropy, values = agent.get_action_and_value(obs_tensor)\n",
    "\n",
    "            action_dict = unbatchify_actions(actions)\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(action_dict)\n",
    "\n",
    "            rb_obs.append(obs_tensor)\n",
    "            rb_actions.append(actions)\n",
    "            rb_logprobs.append(logprobs)\n",
    "            rb_values.append(values)\n",
    "            \n",
    "\n",
    "            r0, r1 = rewards[\"player_0\"], rewards[\"player_1\"]\n",
    "            trunc0, trunc1 = truncs[\"player_0\"], truncs[\"player_1\"]\n",
    "            term0, term1 = terms[\"player_0\"], terms[\"player_1\"]\n",
    "            next_done = torch.tensor([np.logical_or(trunc0, term0), np.logical_or(trunc1, term1)])\n",
    "            rb_rewards.append(torch.tensor([r0, r1], device=device))\n",
    "            rb_dones.append(next_done)\n",
    "\n",
    "            total_episodic_return += np.array([r0, r1])\n",
    "            end_step = step + 1\n",
    "\n",
    "            if all(terms.values()) or all(truncs.values()):\n",
    "                break\n",
    "\n",
    "        # 2. Bootstrapping if not done\n",
    "        with torch.no_grad():\n",
    "            if not all(terms.values()):\n",
    "                final_obs_tensor = batchify_obs(next_obs)\n",
    "                _, _, _, next_values = agent.get_action_and_value(final_obs_tensor)\n",
    "            else:\n",
    "                next_values = torch.zeros(2, device=device)\n",
    "\n",
    "        # 3. Convert lists -> stacked Tensors\n",
    "\n",
    "        num_steps = len(rb_obs)\n",
    "        stacked_obs = {}\n",
    "        for key in rb_obs[0].keys():\n",
    "            cat_list = [step_dict[key] for step_dict in rb_obs]\n",
    "            stacked_obs[key] = torch.stack(cat_list, dim=0)\n",
    "\n",
    "        rb_actions  = torch.stack(rb_actions, dim=0)   # [num_steps, 2, (action_dim)]\n",
    "        rb_logprobs = torch.stack(rb_logprobs, dim=0) # [num_steps, 2]\n",
    "        rb_values   = torch.stack(rb_values, dim=0)   # [num_steps, 2]\n",
    "        rb_rewards  = torch.stack(rb_rewards, dim=0)  # [num_steps, 2]\n",
    "        rb_dones    = torch.stack(rb_dones, dim=0)    # [num_steps, 2]\n",
    "\n",
    "        # 4. GAE or simple advantage\n",
    "        rb_advantages = torch.zeros_like(rb_rewards)\n",
    "        rb_returns = torch.zeros_like(rb_rewards)\n",
    "        gae = torch.zeros(2, device=device)\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                next_val = next_values\n",
    "                done_mask = 1.0 - rb_dones[t].float()\n",
    "            else:\n",
    "                next_val = rb_values[t + 1]\n",
    "                done_mask = 1.0 - rb_dones[t + 1].float()\n",
    "\n",
    "            delta = rb_rewards[t] + gamma * next_val * done_mask - rb_values[t]\n",
    "            gae = delta + gamma * gae_lambda * gae * done_mask\n",
    "            rb_advantages[t] = gae\n",
    "            rb_returns[t] = gae + rb_values[t]\n",
    "\n",
    "        # 5. Flatten time & agent\n",
    "        b_obs = {}\n",
    "        for key, val in stacked_obs.items():\n",
    "            b_obs[key] = val.view(num_steps * 2, *val.shape[2:])\n",
    "\n",
    "        b_actions    = rb_actions.view(num_steps * 2, -1)\n",
    "        b_logprobs   = rb_logprobs.view(num_steps * 2)\n",
    "        b_values     = rb_values.view(num_steps * 2)\n",
    "        b_advantages = rb_advantages.view(num_steps * 2)\n",
    "        b_returns    = rb_returns.view(num_steps * 2)\n",
    "\n",
    "        # We'll track these for logging outside the minibatch loop\n",
    "        clip_fracs = []\n",
    "        old_approx_kls = []\n",
    "        approx_kls = []\n",
    "        last_v_loss = 0.0\n",
    "        last_pg_loss = 0.0\n",
    "\n",
    "        # 6. PPO update\n",
    "        total_batch = num_steps * 2\n",
    "        indices = np.arange(total_batch)\n",
    "        for _ in range(train_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, total_batch, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_inds = indices[start:end]\n",
    "\n",
    "                mb_obs = {k: v[batch_inds] for k, v in b_obs.items()}\n",
    "                mb_actions = b_actions[batch_inds]\n",
    "                mb_old_logprob = b_logprobs[batch_inds]\n",
    "                mb_adv = b_advantages[batch_inds]\n",
    "                mb_returns = b_returns[batch_inds]\n",
    "                mb_values = b_values[batch_inds]\n",
    "                 # Evaluate new logprob\n",
    "                _, new_logprob, entropy, value = agent.get_action_and_value(\n",
    "                    mb_obs, action=mb_actions\n",
    "                )\n",
    "                logratio = new_logprob - mb_old_logprob\n",
    "                ratio = logratio.exp()\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "                # Record them\n",
    "                old_approx_kls.append(old_approx_kl.item())\n",
    "                approx_kls.append(approx_kl.item())\n",
    "\n",
    "                # Compute clip fraction\n",
    "                clip_fraction = ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                clip_fracs.append(clip_fraction)\n",
    "\n",
    "                # Normalize advantages\n",
    "                mb_adv = (mb_adv - mb_adv.mean()) / (mb_adv.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_adv * ratio\n",
    "                pg_loss2 = -mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss (clipped)\n",
    "                value = value.view(-1)\n",
    "                v_loss_unclipped = (value - mb_returns) ** 2\n",
    "                v_clipped = mb_values + torch.clamp(value - mb_values, -clip_coef, clip_coef)\n",
    "                v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
    "                v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
    "\n",
    "                # Entropy\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss + vf_coef * v_loss - ent_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                # We'll keep track of the last minibatch losses for logging\n",
    "                last_v_loss = v_loss.item()\n",
    "                last_pg_loss = pg_loss.item()\n",
    "\n",
    "        # 7. Explained Variance\n",
    "        y_pred = b_values.detach().cpu().numpy()\n",
    "        y_true = b_returns.detach().cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        run.log({\n",
    "            \"episode\": episode,\n",
    "            \"episode_length\": end_step,\n",
    "            \"player0_return\": np.mean(total_episodic_return[0]),\n",
    "            \"player1_return\": np.mean(total_episodic_return[1]),\n",
    "            \"policy_loss\": -last_pg_loss,\n",
    "            \"value_loss\": last_v_loss,\n",
    "            \"old_approx_kl\": np.mean(old_approx_kls),\n",
    "            \"approx_kl\": np.mean(approx_kls),\n",
    "            \"clip_fraction\": np.mean(clip_fracs),\n",
    "            \"explained_variance\": explained_var\n",
    "        })\n",
    "\n",
    "        # 8. Logging (similar structure to your snippet)\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {np.mean(total_episodic_return[0])}\") # zero sum game, so log only 0th agent returns\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {last_v_loss}\")\n",
    "        print(f\"Policy Loss: {last_pg_loss}\")\n",
    "        print(f\"Old Approx KL: {np.mean(old_approx_kls)}\")\n",
    "        print(f\"Approx KL: {np.mean(approx_kls)}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    torch.save(agent.state_dict(), f\"./checkpoints/model_ep{episode}.pt\")\n",
    "    # Upload to wandb\n",
    "    wandb.save(f\"model_ep{episode}.pt\")\n",
    "    run.finish()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "train_ppo(LuxAICAECEnv(LuxAIS3GymEnv(numpy_output=True)), total_episodes=1000, rollout_num_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f06b7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/vzk3b09174b7wm89wghv48jm0000gn/T/ipykernel_91594/2407831279.py:329: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  next_done = torch.tensor([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episode 10 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 1.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Exploiter 0 win rate: 0.6667\n",
      "Exploiter 1 win rate: 0.0000\n",
      "Exploiter 2 win rate: 0.5000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 20 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 811.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5000\n",
      "Exploiter 0 win rate: 0.6000\n",
      "Exploiter 1 win rate: 0.4000\n",
      "Exploiter 2 win rate: 0.6000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 30 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.7500\n",
      "Exploiter 0 win rate: 0.6250\n",
      "Exploiter 1 win rate: 0.5000\n",
      "Exploiter 2 win rate: 0.5714\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 40 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: self\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.8000\n",
      "Exploiter 0 win rate: 0.6000\n",
      "Exploiter 1 win rate: 0.5000\n",
      "Exploiter 2 win rate: 0.6000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 50 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.8333\n",
      "Exploiter 0 win rate: 0.6923\n",
      "Exploiter 1 win rate: 0.5385\n",
      "Exploiter 2 win rate: 0.5833\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 60 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6250\n",
      "Exploiter 0 win rate: 0.6667\n",
      "Exploiter 1 win rate: 0.5333\n",
      "Exploiter 2 win rate: 0.6000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 70 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5556\n",
      "Exploiter 0 win rate: 0.6667\n",
      "Exploiter 1 win rate: 0.4444\n",
      "Exploiter 2 win rate: 0.6471\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 80 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 811.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5455\n",
      "Exploiter 0 win rate: 0.6500\n",
      "Exploiter 1 win rate: 0.4500\n",
      "Exploiter 2 win rate: 0.6000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 90 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 0.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5833\n",
      "Exploiter 0 win rate: 0.6522\n",
      "Exploiter 1 win rate: 0.4348\n",
      "Exploiter 2 win rate: 0.5455\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 100\n",
      "\n",
      "--- Episode 100 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: self\n",
      "Episode return: 203.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5385\n",
      "Exploiter 0 win rate: 0.6400\n",
      "Exploiter 1 win rate: 0.4400\n",
      "Exploiter 2 win rate: 0.5600\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 110 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 0.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.4667\n",
      "Exploiter 0 win rate: 0.6071\n",
      "Exploiter 1 win rate: 0.4286\n",
      "Exploiter 2 win rate: 0.5926\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 120 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 204.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.4667\n",
      "Main win rate vs snapshots: 0.0000\n",
      "Exploiter 0 win rate: 0.6000\n",
      "Exploiter 1 win rate: 0.4667\n",
      "Exploiter 2 win rate: 0.6333\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 130 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 608.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5000\n",
      "Main win rate vs snapshots: 0.0000\n",
      "Exploiter 0 win rate: 0.5758\n",
      "Exploiter 1 win rate: 0.4848\n",
      "Exploiter 2 win rate: 0.6562\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 140 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5789\n",
      "Main win rate vs snapshots: 0.0000\n",
      "Exploiter 0 win rate: 0.5714\n",
      "Exploiter 1 win rate: 0.4857\n",
      "Exploiter 2 win rate: 0.6286\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 150 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5500\n",
      "Main win rate vs snapshots: 0.2000\n",
      "Exploiter 0 win rate: 0.5789\n",
      "Exploiter 1 win rate: 0.5263\n",
      "Exploiter 2 win rate: 0.6216\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 160 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 711.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5455\n",
      "Main win rate vs snapshots: 0.1667\n",
      "Exploiter 0 win rate: 0.5500\n",
      "Exploiter 1 win rate: 0.5500\n",
      "Exploiter 2 win rate: 0.6250\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 170 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 305.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5455\n",
      "Main win rate vs snapshots: 0.1250\n",
      "Exploiter 0 win rate: 0.5116\n",
      "Exploiter 1 win rate: 0.5349\n",
      "Exploiter 2 win rate: 0.5952\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 180 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 608.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5833\n",
      "Main win rate vs snapshots: 0.1111\n",
      "Exploiter 0 win rate: 0.5111\n",
      "Exploiter 1 win rate: 0.5556\n",
      "Exploiter 2 win rate: 0.6000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 190 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 609.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6154\n",
      "Main win rate vs snapshots: 0.1111\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5417\n",
      "Exploiter 2 win rate: 0.5957\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 200\n",
      "\n",
      "--- Episode 200 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 912.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6071\n",
      "Main win rate vs snapshots: 0.1000\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5200\n",
      "Exploiter 2 win rate: 0.6000\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 210 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 812.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6071\n",
      "Main win rate vs snapshots: 0.2500\n",
      "Exploiter 0 win rate: 0.5094\n",
      "Exploiter 1 win rate: 0.5472\n",
      "Exploiter 2 win rate: 0.5962\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 220 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6207\n",
      "Main win rate vs snapshots: 0.3571\n",
      "Exploiter 0 win rate: 0.5091\n",
      "Exploiter 1 win rate: 0.5636\n",
      "Exploiter 2 win rate: 0.5818\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 230 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 407.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5806\n",
      "Main win rate vs snapshots: 0.3571\n",
      "Exploiter 0 win rate: 0.5172\n",
      "Exploiter 1 win rate: 0.5517\n",
      "Exploiter 2 win rate: 0.5965\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 240 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 407.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5625\n",
      "Main win rate vs snapshots: 0.4375\n",
      "Exploiter 0 win rate: 0.5167\n",
      "Exploiter 1 win rate: 0.5667\n",
      "Exploiter 2 win rate: 0.5667\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 250 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 507.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5625\n",
      "Main win rate vs snapshots: 0.3889\n",
      "Exploiter 0 win rate: 0.5079\n",
      "Exploiter 1 win rate: 0.5556\n",
      "Exploiter 2 win rate: 0.5645\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 260 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 305.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5588\n",
      "Main win rate vs snapshots: 0.3684\n",
      "Exploiter 0 win rate: 0.5077\n",
      "Exploiter 1 win rate: 0.5538\n",
      "Exploiter 2 win rate: 0.5846\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 270 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5278\n",
      "Main win rate vs snapshots: 0.3684\n",
      "Exploiter 0 win rate: 0.4853\n",
      "Exploiter 1 win rate: 0.5441\n",
      "Exploiter 2 win rate: 0.5970\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 280 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 610.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5405\n",
      "Main win rate vs snapshots: 0.3810\n",
      "Exploiter 0 win rate: 0.4857\n",
      "Exploiter 1 win rate: 0.5571\n",
      "Exploiter 2 win rate: 0.5857\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 290 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 1.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5526\n",
      "Main win rate vs snapshots: 0.3636\n",
      "Exploiter 0 win rate: 0.5068\n",
      "Exploiter 1 win rate: 0.5479\n",
      "Exploiter 2 win rate: 0.5694\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 300\n",
      "\n",
      "--- Episode 300 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 609.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5641\n",
      "Main win rate vs snapshots: 0.3750\n",
      "Exploiter 0 win rate: 0.5067\n",
      "Exploiter 1 win rate: 0.5600\n",
      "Exploiter 2 win rate: 0.5467\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 310 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5750\n",
      "Main win rate vs snapshots: 0.3600\n",
      "Exploiter 0 win rate: 0.4872\n",
      "Exploiter 1 win rate: 0.5641\n",
      "Exploiter 2 win rate: 0.5455\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 320 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 507.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5714\n",
      "Main win rate vs snapshots: 0.3462\n",
      "Exploiter 0 win rate: 0.4875\n",
      "Exploiter 1 win rate: 0.5625\n",
      "Exploiter 2 win rate: 0.5625\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 330 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5682\n",
      "Main win rate vs snapshots: 0.3462\n",
      "Exploiter 0 win rate: 0.4819\n",
      "Exploiter 1 win rate: 0.5542\n",
      "Exploiter 2 win rate: 0.5610\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 340 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 204.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5682\n",
      "Main win rate vs snapshots: 0.3103\n",
      "Exploiter 0 win rate: 0.4824\n",
      "Exploiter 1 win rate: 0.5412\n",
      "Exploiter 2 win rate: 0.5647\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 350 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5778\n",
      "Main win rate vs snapshots: 0.3333\n",
      "Exploiter 0 win rate: 0.4886\n",
      "Exploiter 1 win rate: 0.5568\n",
      "Exploiter 2 win rate: 0.5632\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 360 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 508.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5957\n",
      "Main win rate vs snapshots: 0.3226\n",
      "Exploiter 0 win rate: 0.4778\n",
      "Exploiter 1 win rate: 0.5556\n",
      "Exploiter 2 win rate: 0.5444\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 370 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 407.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6122\n",
      "Main win rate vs snapshots: 0.3226\n",
      "Exploiter 0 win rate: 0.4731\n",
      "Exploiter 1 win rate: 0.5484\n",
      "Exploiter 2 win rate: 0.5543\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 380 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 306.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6078\n",
      "Main win rate vs snapshots: 0.3125\n",
      "Exploiter 0 win rate: 0.4632\n",
      "Exploiter 1 win rate: 0.5368\n",
      "Exploiter 2 win rate: 0.5684\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 390 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6154\n",
      "Main win rate vs snapshots: 0.3333\n",
      "Exploiter 0 win rate: 0.4592\n",
      "Exploiter 1 win rate: 0.5306\n",
      "Exploiter 2 win rate: 0.5670\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 400\n",
      "\n",
      "--- Episode 400 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 102.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6296\n",
      "Main win rate vs snapshots: 0.3235\n",
      "Exploiter 0 win rate: 0.4600\n",
      "Exploiter 1 win rate: 0.5400\n",
      "Exploiter 2 win rate: 0.5600\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 410 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 306.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6296\n",
      "Main win rate vs snapshots: 0.3611\n",
      "Exploiter 0 win rate: 0.4563\n",
      "Exploiter 1 win rate: 0.5437\n",
      "Exploiter 2 win rate: 0.5490\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 420 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 1014.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6182\n",
      "Main win rate vs snapshots: 0.3684\n",
      "Exploiter 0 win rate: 0.4571\n",
      "Exploiter 1 win rate: 0.5524\n",
      "Exploiter 2 win rate: 0.5429\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 430 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 305.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6071\n",
      "Main win rate vs snapshots: 0.3590\n",
      "Exploiter 0 win rate: 0.4630\n",
      "Exploiter 1 win rate: 0.5556\n",
      "Exploiter 2 win rate: 0.5421\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 440 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6140\n",
      "Main win rate vs snapshots: 0.3902\n",
      "Exploiter 0 win rate: 0.4636\n",
      "Exploiter 1 win rate: 0.5636\n",
      "Exploiter 2 win rate: 0.5273\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 450 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 610.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.6140\n",
      "Main win rate vs snapshots: 0.3721\n",
      "Exploiter 0 win rate: 0.4690\n",
      "Exploiter 1 win rate: 0.5664\n",
      "Exploiter 2 win rate: 0.5268\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 460 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 405.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6102\n",
      "Main win rate vs snapshots: 0.3864\n",
      "Exploiter 0 win rate: 0.4609\n",
      "Exploiter 1 win rate: 0.5739\n",
      "Exploiter 2 win rate: 0.5217\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 470 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 507.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6000\n",
      "Main win rate vs snapshots: 0.4000\n",
      "Exploiter 0 win rate: 0.4746\n",
      "Exploiter 1 win rate: 0.5763\n",
      "Exploiter 2 win rate: 0.5299\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 480 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 711.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5968\n",
      "Main win rate vs snapshots: 0.4130\n",
      "Exploiter 0 win rate: 0.4750\n",
      "Exploiter 1 win rate: 0.5750\n",
      "Exploiter 2 win rate: 0.5250\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 490 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 812.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5873\n",
      "Main win rate vs snapshots: 0.4043\n",
      "Exploiter 0 win rate: 0.4715\n",
      "Exploiter 1 win rate: 0.5854\n",
      "Exploiter 2 win rate: 0.5164\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 500\n",
      "\n",
      "--- Episode 500 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 306.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5758\n",
      "Main win rate vs snapshots: 0.4043\n",
      "Exploiter 0 win rate: 0.4800\n",
      "Exploiter 1 win rate: 0.5920\n",
      "Exploiter 2 win rate: 0.5200\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 510 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 405.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5758\n",
      "Main win rate vs snapshots: 0.4082\n",
      "Exploiter 0 win rate: 0.4766\n",
      "Exploiter 1 win rate: 0.5781\n",
      "Exploiter 2 win rate: 0.5197\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 520 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 609.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5821\n",
      "Main win rate vs snapshots: 0.4118\n",
      "Exploiter 0 win rate: 0.4769\n",
      "Exploiter 1 win rate: 0.5692\n",
      "Exploiter 2 win rate: 0.5154\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 530 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5735\n",
      "Main win rate vs snapshots: 0.4038\n",
      "Exploiter 0 win rate: 0.4737\n",
      "Exploiter 1 win rate: 0.5714\n",
      "Exploiter 2 win rate: 0.5227\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 540 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 812.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5857\n",
      "Main win rate vs snapshots: 0.4151\n",
      "Exploiter 0 win rate: 0.4815\n",
      "Exploiter 1 win rate: 0.5630\n",
      "Exploiter 2 win rate: 0.5259\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 550 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 406.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5833\n",
      "Main win rate vs snapshots: 0.4151\n",
      "Exploiter 0 win rate: 0.4855\n",
      "Exploiter 1 win rate: 0.5580\n",
      "Exploiter 2 win rate: 0.5255\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 560 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 406.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5811\n",
      "Main win rate vs snapshots: 0.4074\n",
      "Exploiter 0 win rate: 0.4857\n",
      "Exploiter 1 win rate: 0.5500\n",
      "Exploiter 2 win rate: 0.5357\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 570 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 610.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5733\n",
      "Main win rate vs snapshots: 0.4000\n",
      "Exploiter 0 win rate: 0.4895\n",
      "Exploiter 1 win rate: 0.5524\n",
      "Exploiter 2 win rate: 0.5352\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 580 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 1014.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5844\n",
      "Main win rate vs snapshots: 0.4107\n",
      "Exploiter 0 win rate: 0.4897\n",
      "Exploiter 1 win rate: 0.5517\n",
      "Exploiter 2 win rate: 0.5241\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 590 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5844\n",
      "Main win rate vs snapshots: 0.4310\n",
      "Exploiter 0 win rate: 0.4865\n",
      "Exploiter 1 win rate: 0.5541\n",
      "Exploiter 2 win rate: 0.5238\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 600\n",
      "\n",
      "--- Episode 600 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5897\n",
      "Main win rate vs snapshots: 0.4333\n",
      "Exploiter 0 win rate: 0.4867\n",
      "Exploiter 1 win rate: 0.5600\n",
      "Exploiter 2 win rate: 0.5200\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 610 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5897\n",
      "Main win rate vs snapshots: 0.4516\n",
      "Exploiter 0 win rate: 0.4771\n",
      "Exploiter 1 win rate: 0.5621\n",
      "Exploiter 2 win rate: 0.5197\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 620 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 711.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5926\n",
      "Main win rate vs snapshots: 0.4516\n",
      "Exploiter 0 win rate: 0.4839\n",
      "Exploiter 1 win rate: 0.5548\n",
      "Exploiter 2 win rate: 0.5226\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 630 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 204.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5976\n",
      "Main win rate vs snapshots: 0.4603\n",
      "Exploiter 0 win rate: 0.4810\n",
      "Exploiter 1 win rate: 0.5443\n",
      "Exploiter 2 win rate: 0.5223\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 640 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 507.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.6024\n",
      "Main win rate vs snapshots: 0.4615\n",
      "Exploiter 0 win rate: 0.4875\n",
      "Exploiter 1 win rate: 0.5437\n",
      "Exploiter 2 win rate: 0.5188\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 650 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5882\n",
      "Main win rate vs snapshots: 0.4615\n",
      "Exploiter 0 win rate: 0.4847\n",
      "Exploiter 1 win rate: 0.5460\n",
      "Exploiter 2 win rate: 0.5247\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 660 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 709.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5862\n",
      "Main win rate vs snapshots: 0.4697\n",
      "Exploiter 0 win rate: 0.4848\n",
      "Exploiter 1 win rate: 0.5515\n",
      "Exploiter 2 win rate: 0.5212\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 670 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 812.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5909\n",
      "Main win rate vs snapshots: 0.4627\n",
      "Exploiter 0 win rate: 0.4881\n",
      "Exploiter 1 win rate: 0.5476\n",
      "Exploiter 2 win rate: 0.5269\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 680 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 0.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5778\n",
      "Main win rate vs snapshots: 0.4706\n",
      "Exploiter 0 win rate: 0.4941\n",
      "Exploiter 1 win rate: 0.5412\n",
      "Exploiter 2 win rate: 0.5294\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 690 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 508.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5761\n",
      "Main win rate vs snapshots: 0.4706\n",
      "Exploiter 0 win rate: 0.4913\n",
      "Exploiter 1 win rate: 0.5434\n",
      "Exploiter 2 win rate: 0.5349\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 700\n",
      "\n",
      "--- Episode 700 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 203.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5699\n",
      "Main win rate vs snapshots: 0.4857\n",
      "Exploiter 0 win rate: 0.4914\n",
      "Exploiter 1 win rate: 0.5486\n",
      "Exploiter 2 win rate: 0.5314\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 710 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 204.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5684\n",
      "Main win rate vs snapshots: 0.4857\n",
      "Exploiter 0 win rate: 0.4944\n",
      "Exploiter 1 win rate: 0.5449\n",
      "Exploiter 2 win rate: 0.5367\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 720 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 1015.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5567\n",
      "Main win rate vs snapshots: 0.4930\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5444\n",
      "Exploiter 2 win rate: 0.5333\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 730 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5567\n",
      "Main win rate vs snapshots: 0.4795\n",
      "Exploiter 0 win rate: 0.5027\n",
      "Exploiter 1 win rate: 0.5464\n",
      "Exploiter 2 win rate: 0.5330\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 740 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 710.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5612\n",
      "Main win rate vs snapshots: 0.4667\n",
      "Exploiter 0 win rate: 0.5027\n",
      "Exploiter 1 win rate: 0.5514\n",
      "Exploiter 2 win rate: 0.5297\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 750 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 610.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5500\n",
      "Main win rate vs snapshots: 0.4667\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5585\n",
      "Exploiter 2 win rate: 0.5348\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 760 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 508.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5545\n",
      "Main win rate vs snapshots: 0.4545\n",
      "Exploiter 0 win rate: 0.4947\n",
      "Exploiter 1 win rate: 0.5579\n",
      "Exploiter 2 win rate: 0.5316\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 770 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 508.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5490\n",
      "Main win rate vs snapshots: 0.4487\n",
      "Exploiter 0 win rate: 0.4974\n",
      "Exploiter 1 win rate: 0.5596\n",
      "Exploiter 2 win rate: 0.5312\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 780 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 508.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5437\n",
      "Main win rate vs snapshots: 0.4500\n",
      "Exploiter 0 win rate: 0.4923\n",
      "Exploiter 1 win rate: 0.5641\n",
      "Exploiter 2 win rate: 0.5282\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 790 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 609.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5385\n",
      "Main win rate vs snapshots: 0.4444\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5606\n",
      "Exploiter 2 win rate: 0.5279\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 800\n",
      "\n",
      "--- Episode 800 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 1015.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5472\n",
      "Main win rate vs snapshots: 0.4512\n",
      "Exploiter 0 win rate: 0.4950\n",
      "Exploiter 1 win rate: 0.5550\n",
      "Exploiter 2 win rate: 0.5300\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 810 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5421\n",
      "Main win rate vs snapshots: 0.4578\n",
      "Exploiter 0 win rate: 0.5025\n",
      "Exploiter 1 win rate: 0.5468\n",
      "Exploiter 2 win rate: 0.5297\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 820 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 406.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5463\n",
      "Main win rate vs snapshots: 0.4471\n",
      "Exploiter 0 win rate: 0.5024\n",
      "Exploiter 1 win rate: 0.5512\n",
      "Exploiter 2 win rate: 0.5317\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 830 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 1014.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5413\n",
      "Main win rate vs snapshots: 0.4535\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5529\n",
      "Exploiter 2 win rate: 0.5314\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 840 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 812.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5455\n",
      "Main win rate vs snapshots: 0.4659\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5476\n",
      "Exploiter 2 win rate: 0.5286\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 850 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5405\n",
      "Main win rate vs snapshots: 0.4607\n",
      "Exploiter 0 win rate: 0.5070\n",
      "Exploiter 1 win rate: 0.5493\n",
      "Exploiter 2 win rate: 0.5236\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 860 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 610.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5357\n",
      "Main win rate vs snapshots: 0.4615\n",
      "Exploiter 0 win rate: 0.5023\n",
      "Exploiter 1 win rate: 0.5535\n",
      "Exploiter 2 win rate: 0.5256\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 870 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 811.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5357\n",
      "Main win rate vs snapshots: 0.4516\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5596\n",
      "Exploiter 2 win rate: 0.5253\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 880 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 305.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5398\n",
      "Main win rate vs snapshots: 0.4421\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5591\n",
      "Exploiter 2 win rate: 0.5227\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 890 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 811.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5391\n",
      "Main win rate vs snapshots: 0.4421\n",
      "Exploiter 0 win rate: 0.4933\n",
      "Exploiter 1 win rate: 0.5605\n",
      "Exploiter 2 win rate: 0.5270\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 900\n",
      "\n",
      "--- Episode 900 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 912.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5345\n",
      "Main win rate vs snapshots: 0.4433\n",
      "Exploiter 0 win rate: 0.4889\n",
      "Exploiter 1 win rate: 0.5600\n",
      "Exploiter 2 win rate: 0.5333\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 910 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 912.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5345\n",
      "Main win rate vs snapshots: 0.4444\n",
      "Exploiter 0 win rate: 0.4912\n",
      "Exploiter 1 win rate: 0.5614\n",
      "Exploiter 2 win rate: 0.5374\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 920 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 913.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5339\n",
      "Main win rate vs snapshots: 0.4400\n",
      "Exploiter 0 win rate: 0.4913\n",
      "Exploiter 1 win rate: 0.5565\n",
      "Exploiter 2 win rate: 0.5348\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 930 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 609.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5294\n",
      "Main win rate vs snapshots: 0.4455\n",
      "Exploiter 0 win rate: 0.4979\n",
      "Exploiter 1 win rate: 0.5579\n",
      "Exploiter 2 win rate: 0.5388\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 940 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: strongest_exploiter\n",
      "Episode return: 406.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5250\n",
      "Main win rate vs snapshots: 0.4563\n",
      "Exploiter 0 win rate: 0.5021\n",
      "Exploiter 1 win rate: 0.5574\n",
      "Exploiter 2 win rate: 0.5404\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 950 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 609.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5250\n",
      "Main win rate vs snapshots: 0.4571\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5546\n",
      "Exploiter 2 win rate: 0.5401\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 960 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_exploiter\n",
      "Episode return: 204.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5164\n",
      "Main win rate vs snapshots: 0.4623\n",
      "Exploiter 0 win rate: 0.5000\n",
      "Exploiter 1 win rate: 0.5542\n",
      "Exploiter 2 win rate: 0.5375\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 970 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 103.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5164\n",
      "Main win rate vs snapshots: 0.4722\n",
      "Exploiter 0 win rate: 0.4938\n",
      "Exploiter 1 win rate: 0.5514\n",
      "Exploiter 2 win rate: 0.5372\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 980 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: random_snapshot\n",
      "Episode return: 1.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5203\n",
      "Main win rate vs snapshots: 0.4727\n",
      "Exploiter 0 win rate: 0.4980\n",
      "Exploiter 1 win rate: 0.5510\n",
      "Exploiter 2 win rate: 0.5306\n",
      "----------------------------\n",
      "\n",
      "\n",
      "--- Episode 990 Summary ---\n",
      "Agent type: exploiter, Agent ID: 1\n",
      "Opponent type: main\n",
      "Episode return: 406.0000\n",
      "Episode length: 505\n",
      "Win: False\n",
      "Main win rate vs exploiters: 0.5203\n",
      "Main win rate vs snapshots: 0.4821\n",
      "Exploiter 0 win rate: 0.4919\n",
      "Exploiter 1 win rate: 0.5444\n",
      "Exploiter 2 win rate: 0.5304\n",
      "----------------------------\n",
      "\n",
      "Created snapshot at episode 1000\n",
      "\n",
      "--- Episode 1000 Summary ---\n",
      "Agent type: main, Agent ID: 0\n",
      "Opponent type: recent_snapshot\n",
      "Episode return: 508.0000\n",
      "Episode length: 505\n",
      "Win: True\n",
      "Main win rate vs exploiters: 0.5242\n",
      "Main win rate vs snapshots: 0.4825\n",
      "Exploiter 0 win rate: 0.4880\n",
      "Exploiter 1 win rate: 0.5440\n",
      "Exploiter 2 win rate: 0.5280\n",
      "----------------------------\n",
      "\n",
      "League training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (net): MultiInputActorCriticPolicy(\n",
       "    (features_extractor): CombinedExtractor(\n",
       "      (extractors): ModuleDict(\n",
       "        (env_cfg_map_height): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_map_width): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_max_steps_in_match): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "        (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "        (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "        (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "        (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "        (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "        (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "        (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_position): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (pi_features_extractor): CombinedExtractor(\n",
       "      (extractors): ModuleDict(\n",
       "        (env_cfg_map_height): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_map_width): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_max_steps_in_match): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "        (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "        (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "        (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "        (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "        (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "        (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "        (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_position): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (vf_features_extractor): CombinedExtractor(\n",
       "      (extractors): ModuleDict(\n",
       "        (env_cfg_map_height): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_map_width): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_max_steps_in_match): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "        (env_cfg_unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "        (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "        (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "        (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "        (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "        (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "        (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "        (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "        (units_position): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (mlp_extractor): MlpExtractor(\n",
       "      (policy_net): Sequential(\n",
       "        (0): Linear(in_features=1886, out_features=64, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): Tanh()\n",
       "      )\n",
       "      (value_net): Sequential(\n",
       "        (0): Linear(in_features=1886, out_features=64, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (action_net): Linear(in_features=64, out_features=96, bias=True)\n",
       "    (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "def train_with_league(env, total_episodes=1000, rollout_num_steps=512, \n",
    "                      snapshot_freq=50, num_exploiters=2):\n",
    "    \"\"\"\n",
    "    Train PPO agents using a simplified AlphaStar-like league system\n",
    "    \n",
    "    Args:\n",
    "        env: Environment to train in\n",
    "        total_episodes: Total episodes to train for\n",
    "        rollout_num_steps: Number of steps per rollout\n",
    "        snapshot_freq: How often to take snapshots of the main agent\n",
    "        num_exploiters: Number of exploiter agents to maintain\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    train_epochs = 4\n",
    "    gae_lambda = 0.95\n",
    "    max_grad_norm = 0.5\n",
    "    lr = 2.5e-4\n",
    "    seed = 2025\n",
    "    anneal_lr = True\n",
    "    \n",
    "    # Initialize wandb\n",
    "    run = wandb.init(\n",
    "        entity=\"ay2425s2-cs3263-group-13\",\n",
    "        project=\"lux-alphastar-league\",\n",
    "        config={\n",
    "            \"ent_coef\": ent_coef,\n",
    "            \"vf_coef\": vf_coef,\n",
    "            \"clip_coef\": clip_coef,\n",
    "            \"gamma\": gamma,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train_epochs\": train_epochs,\n",
    "            \"total_episodes\": total_episodes,\n",
    "            \"max_steps_per_episode\": rollout_num_steps,\n",
    "            \"num_exploiters\": num_exploiters,\n",
    "        },\n",
    "        save_code=True,\n",
    "    )\n",
    "    \n",
    "    # Create checkpoints directory\n",
    "    Path(\"./checkpoints\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize league components\n",
    "    \n",
    "    # 1. Main agent - our best agent that trains against everyone\n",
    "    main_agent = Agent(env).to(device)\n",
    "    main_optimizer = optim.Adam(main_agent.parameters(), lr=lr, eps=1e-5)\n",
    "    \n",
    "    # 2. Exploiter agents - specialize in exploiting the main agent\n",
    "    exploiters = []\n",
    "    for i in range(num_exploiters):\n",
    "        exploiter = {\n",
    "            \"agent\": Agent(env).to(device),\n",
    "            \"optimizer\": optim.Adam(main_agent.parameters(), lr=lr, eps=1e-5),\n",
    "            \"id\": i,\n",
    "            \"episodes_trained\": 0,\n",
    "            \"wins_against_main\": 0,\n",
    "            \"matches_against_main\": 0\n",
    "        }\n",
    "        exploiters.append(exploiter)\n",
    "    \n",
    "    # 3. Historical snapshots of the main agent\n",
    "    snapshots = []  # Will store (episode_number, model_path) tuples\n",
    "    \n",
    "    # Track main agent's performance against exploiters and snapshots\n",
    "    main_agent_stats = {\n",
    "        \"wins_vs_exploiters\": 0,\n",
    "        \"matches_vs_exploiters\": 0,\n",
    "        \"wins_vs_snapshots\": 0,\n",
    "        \"matches_vs_snapshots\": 0\n",
    "    }\n",
    "    \n",
    "    # Main training loop\n",
    "    for episode in range(1, total_episodes + 1):\n",
    "        # Determine which agent to train this episode\n",
    "        if episode % (num_exploiters + 1) == 0:\n",
    "            # Train main agent\n",
    "            current_agent = main_agent\n",
    "            current_optimizer = main_optimizer\n",
    "            agent_type = \"main\"\n",
    "            agent_id = 0\n",
    "        else:\n",
    "            # Train exploiter agent\n",
    "            exploiter_idx = (episode % (num_exploiters + 1)) - 1\n",
    "            current_agent = exploiters[exploiter_idx][\"agent\"]\n",
    "            current_optimizer = exploiters[exploiter_idx][\"optimizer\"]\n",
    "            exploiters[exploiter_idx][\"episodes_trained\"] += 1\n",
    "            agent_type = \"exploiter\"\n",
    "            agent_id = exploiter_idx\n",
    "        \n",
    "        # Apply learning rate annealing if enabled\n",
    "        if anneal_lr:\n",
    "            frac = 1.0 - (episode - 1.0) / total_episodes\n",
    "            lrnow = frac * lr\n",
    "            current_optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "        \n",
    "        # Select opponent based on agent type\n",
    "        if agent_type == \"main\":\n",
    "            # Main agent trains against a mix of exploiters and snapshots\n",
    "            if random.random() < 0.5 and exploiters:  # 70% chance to play against exploiters\n",
    "                # Select exploiter, preferring those that win more often\n",
    "                if random.random() < 0.7:  # 80% chance to select strongest exploiter\n",
    "                    # Sort exploiters by win rate against main agent\n",
    "                    sorted_exploiters = sorted(\n",
    "                        exploiters, \n",
    "                        key=lambda x: x[\"wins_against_main\"] / max(1, x[\"matches_against_main\"]),\n",
    "                        reverse=True\n",
    "                    )\n",
    "                    opponent = sorted_exploiters[0][\"agent\"]\n",
    "                    opponent_type = \"strongest_exploiter\"\n",
    "                else:\n",
    "                    # Random exploiter\n",
    "                    opponent = random.choice(exploiters)[\"agent\"]\n",
    "                    opponent_type = \"random_exploiter\"\n",
    "            elif snapshots:  # Otherwise use historical snapshot if available\n",
    "                # 50% chance to play against most recent snapshot, otherwise random snapshot\n",
    "                if random.random() < 0.5:\n",
    "                    opponent_path = snapshots[-1][1]  # Most recent snapshot\n",
    "                    opponent_type = \"recent_snapshot\"\n",
    "                else:\n",
    "                    opponent_path = random.choice(snapshots)[1]  # Random snapshot\n",
    "                    opponent_type = \"random_snapshot\"\n",
    "                \n",
    "                # Load snapshot weights\n",
    "                opponent = Agent(env).to(device)\n",
    "                opponent.load_state_dict(torch.load(opponent_path))\n",
    "            else:\n",
    "                # If no snapshots yet, self-play\n",
    "                opponent = current_agent\n",
    "                opponent_type = \"self\"\n",
    "        else:\n",
    "            # Exploiters only train against the main agent\n",
    "            opponent = main_agent\n",
    "            opponent_type = \"main\"\n",
    "        \n",
    "        # Run a single training episode\n",
    "        metrics = train_league_episode(\n",
    "            env,\n",
    "            current_agent,\n",
    "            opponent,\n",
    "            current_optimizer,\n",
    "            rollout_num_steps=rollout_num_steps,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            clip_coef=clip_coef,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            train_epochs=train_epochs,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed + episode\n",
    "        )\n",
    "        \n",
    "        # Update win statistics\n",
    "        if agent_type == \"main\":\n",
    "            if opponent_type in [\"strongest_exploiter\", \"random_exploiter\"]:\n",
    "                main_agent_stats[\"matches_vs_exploiters\"] += 1\n",
    "                if metrics[\"win\"]:\n",
    "                    main_agent_stats[\"wins_vs_exploiters\"] += 1\n",
    "            elif opponent_type in [\"recent_snapshot\", \"random_snapshot\"]:\n",
    "                main_agent_stats[\"matches_vs_snapshots\"] += 1\n",
    "                if metrics[\"win\"]:\n",
    "                    main_agent_stats[\"wins_vs_snapshots\"] += 1\n",
    "        else:  # exploiter\n",
    "            exploiters[agent_id][\"matches_against_main\"] += 1\n",
    "            if metrics[\"win\"]:\n",
    "                exploiters[agent_id][\"wins_against_main\"] += 1\n",
    "        \n",
    "        # Take snapshot of main agent at regular intervals\n",
    "        if agent_type == \"main\" and episode % snapshot_freq == 0:\n",
    "            snapshot_path = f\"./checkpoints/main_snapshot_ep{episode}.pt\"\n",
    "            torch.save(current_agent.state_dict(), snapshot_path)\n",
    "            snapshots.append((episode, snapshot_path))\n",
    "            print(f\"Created snapshot at episode {episode}\")\n",
    "            \n",
    "            # Keep only the most recent 10 snapshots to manage storage\n",
    "            if len(snapshots) > 10:\n",
    "                oldest = snapshots.pop(0)\n",
    "                # Optionally delete the file to save space\n",
    "                # os.remove(oldest[1])\n",
    "        \n",
    "        # Log metrics\n",
    "        log_data = {\n",
    "            \"episode\": episode,\n",
    "            \"agent_type\": agent_type,\n",
    "            \"agent_id\": agent_id,\n",
    "            \"opponent_type\": opponent_type,\n",
    "            \"episode_length\": metrics[\"episode_length\"],\n",
    "            \"player0_return\": metrics[\"player0_return\"],\n",
    "            \"policy_loss\": metrics[\"policy_loss\"],\n",
    "            \"value_loss\": metrics[\"value_loss\"],\n",
    "            \"win\": metrics[\"win\"],\n",
    "            \"lr\": current_optimizer.param_groups[0][\"lr\"],\n",
    "        }\n",
    "        \n",
    "        # Add league statistics\n",
    "        if main_agent_stats[\"matches_vs_exploiters\"] > 0:\n",
    "            log_data[\"main_win_rate_vs_exploiters\"] = main_agent_stats[\"wins_vs_exploiters\"] / main_agent_stats[\"matches_vs_exploiters\"]\n",
    "        \n",
    "        if main_agent_stats[\"matches_vs_snapshots\"] > 0:\n",
    "            log_data[\"main_win_rate_vs_snapshots\"] = main_agent_stats[\"wins_vs_snapshots\"] / main_agent_stats[\"matches_vs_snapshots\"]\n",
    "        \n",
    "        for i, exploiter in enumerate(exploiters):\n",
    "            if exploiter[\"matches_against_main\"] > 0:\n",
    "                log_data[f\"exploiter{i}_win_rate\"] = exploiter[\"wins_against_main\"] / exploiter[\"matches_against_main\"]\n",
    "        \n",
    "        run.log(log_data)\n",
    "        \n",
    "        # Print summary every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"\\n--- Episode {episode} Summary ---\")\n",
    "            print(f\"Agent type: {agent_type}, Agent ID: {agent_id}\")\n",
    "            print(f\"Opponent type: {opponent_type}\")\n",
    "            print(f\"Episode return: {metrics['player0_return']:.4f}\")\n",
    "            print(f\"Episode length: {metrics['episode_length']}\")\n",
    "            print(f\"Win: {metrics['win']}\")\n",
    "            \n",
    "            if main_agent_stats[\"matches_vs_exploiters\"] > 0:\n",
    "                main_vs_exp = main_agent_stats[\"wins_vs_exploiters\"] / main_agent_stats[\"matches_vs_exploiters\"]\n",
    "                print(f\"Main win rate vs exploiters: {main_vs_exp:.4f}\")\n",
    "            \n",
    "            if main_agent_stats[\"matches_vs_snapshots\"] > 0:\n",
    "                main_vs_snap = main_agent_stats[\"wins_vs_snapshots\"] / main_agent_stats[\"matches_vs_snapshots\"]\n",
    "                print(f\"Main win rate vs snapshots: {main_vs_snap:.4f}\")\n",
    "            \n",
    "            for i, exploiter in enumerate(exploiters):\n",
    "                if exploiter[\"matches_against_main\"] > 0:\n",
    "                    exp_win_rate = exploiter[\"wins_against_main\"] / exploiter[\"matches_against_main\"]\n",
    "                    print(f\"Exploiter {i} win rate: {exp_win_rate:.4f}\")\n",
    "            \n",
    "            print(\"----------------------------\\n\")\n",
    "    \n",
    "    # Save final models\n",
    "    timestamp = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
    "    \n",
    "    # Save main agent\n",
    "    main_path = f\"./checkpoints/main_final_{timestamp}.pt\"\n",
    "    torch.save(main_agent.state_dict(), main_path)\n",
    "    wandb.save(main_path)\n",
    "    \n",
    "    # Save exploiters\n",
    "    for i, exploiter in enumerate(exploiters):\n",
    "        exploiter_path = f\"./checkpoints/exploiter{i}_final_{timestamp}.pt\"\n",
    "        torch.save(exploiter[\"agent\"].state_dict(), exploiter_path)\n",
    "    \n",
    "    run.finish()\n",
    "    print(\"League training complete.\")\n",
    "    return main_agent\n",
    "\n",
    "\n",
    "def train_league_episode(env, agent, opponent, optimizer, rollout_num_steps=512, \n",
    "                        ent_coef=0.1, vf_coef=0.1, clip_coef=0.1,\n",
    "                        gamma=0.99, gae_lambda=0.95, max_grad_norm=0.5,\n",
    "                        train_epochs=4, batch_size=32, seed=2025):\n",
    "    \"\"\"\n",
    "    Run a single training episode for the league training system\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        agent: The agent being trained (player 0)\n",
    "        opponent: The opponent agent (player 1)\n",
    "        optimizer: The optimizer for the agent\n",
    "        Other args: Standard PPO parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics from the episode\n",
    "    \"\"\"\n",
    "    next_obs, info = env.reset(seed=seed)\n",
    "    total_episodic_return = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "    rb_obs = []\n",
    "    rb_actions = []\n",
    "    rb_logprobs = []\n",
    "    rb_rewards = []\n",
    "    rb_dones = []\n",
    "    rb_values = []\n",
    "\n",
    "    end_step = 0\n",
    "\n",
    "    # 1. Collect experience\n",
    "    for step in range(rollout_num_steps):\n",
    "        obs_tensor = batchify_obs(next_obs)\n",
    "        \n",
    "        # Get actions for both agent and opponent\n",
    "        with torch.no_grad():\n",
    "            # Agent (player 0) action\n",
    "            agent_actions, agent_logprobs, agent_entropy, agent_values = agent.get_action_and_value(obs_tensor)\n",
    "            \n",
    "            # Opponent (player 1) action\n",
    "            opponent_actions, _, _, _ = opponent.get_action_and_value(obs_tensor)\n",
    "        \n",
    "        # Combine actions\n",
    "        actions = torch.zeros_like(agent_actions)\n",
    "        actions[0] = agent_actions[0]  # Agent's action for player 0\n",
    "        actions[1] = opponent_actions[1]  # Opponent's action for player 1\n",
    "        \n",
    "        logprobs = torch.zeros_like(agent_logprobs)\n",
    "        logprobs[0] = agent_logprobs[0]  # Only care about agent's logprobs\n",
    "        \n",
    "        values = torch.zeros_like(agent_values)\n",
    "        values[0] = agent_values[0]  # Only care about agent's values\n",
    "        \n",
    "        # Step environment\n",
    "        action_dict = unbatchify_actions(actions)\n",
    "        next_obs, rewards, terms, truncs, infos = env.step(action_dict)\n",
    "\n",
    "        # Store only the agent's experience (player 0)\n",
    "        rb_obs.append(obs_tensor)\n",
    "        rb_actions.append(actions)\n",
    "        rb_logprobs.append(logprobs)\n",
    "        rb_values.append(values)\n",
    "\n",
    "        r0, r1 = rewards[\"player_0\"], rewards[\"player_1\"]\n",
    "        trunc0, trunc1 = truncs[\"player_0\"], truncs[\"player_1\"]\n",
    "        term0, term1 = terms[\"player_0\"], terms[\"player_1\"]\n",
    "        next_done = torch.tensor([\n",
    "            np.logical_or(trunc0, term0),\n",
    "            np.logical_or(trunc1, term1),\n",
    "        ])\n",
    "        rb_rewards.append(torch.tensor([r0, r1], device=device))\n",
    "        rb_dones.append(next_done)\n",
    "\n",
    "        total_episodic_return += np.array([r0, r1])\n",
    "        end_step = step + 1\n",
    "\n",
    "        if all(terms.values()) or all(truncs.values()):\n",
    "            break\n",
    "\n",
    "    # Determine if the agent won (player 0)\n",
    "    won = total_episodic_return[0] > total_episodic_return[1]\n",
    "\n",
    "    # 2. Bootstrap if not done\n",
    "    with torch.no_grad():\n",
    "        if not all(terms.values()):\n",
    "            final_obs_tensor = batchify_obs(next_obs)\n",
    "            _, _, _, next_values = agent.get_action_and_value(final_obs_tensor)\n",
    "        else:\n",
    "            next_values = torch.zeros(2, device=device)\n",
    "\n",
    "    # 3. Convert lists -> Tensors\n",
    "    num_steps = len(rb_obs)\n",
    "    stacked_obs = {}\n",
    "    for key in rb_obs[0].keys():\n",
    "        cat_list = [step_dict[key] for step_dict in rb_obs]\n",
    "        stacked_obs[key] = torch.stack(cat_list, dim=0)\n",
    "\n",
    "    rb_actions = torch.stack(rb_actions, dim=0)\n",
    "    rb_logprobs = torch.stack(rb_logprobs, dim=0)\n",
    "    rb_values = torch.stack(rb_values, dim=0)\n",
    "    rb_rewards = torch.stack(rb_rewards, dim=0)\n",
    "    rb_dones = torch.stack(rb_dones, dim=0)\n",
    "\n",
    "    # 4. GAE calculation - Only compute for player 0 (the agent we're training)\n",
    "    rb_advantages = torch.zeros_like(rb_rewards)\n",
    "    rb_returns = torch.zeros_like(rb_rewards)\n",
    "    gae = torch.zeros(2, device=device)\n",
    "\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            next_val = next_values\n",
    "            done_mask = 1.0 - rb_dones[t].float().to(device)\n",
    "        else:\n",
    "            next_val = rb_values[t + 1]\n",
    "            done_mask = 1.0 - rb_dones[t + 1].float().to(device)\n",
    "\n",
    "        delta = rb_rewards[t] + gamma * next_val * done_mask - rb_values[t]\n",
    "        gae = delta + gamma * gae_lambda * gae * done_mask\n",
    "        rb_advantages[t] = gae\n",
    "        rb_returns[t] = gae + rb_values[t]\n",
    "\n",
    "    # 5. Flatten batch - but only use player 0's data (the agent we're training)\n",
    "    b_obs = {}\n",
    "    for key, val in stacked_obs.items():\n",
    "        # Extract only player 0's observations\n",
    "        b_obs[key] = val[:, 0].unsqueeze(1)  # Keep dimension for compatibility\n",
    "        # b_obs[key] = val[:, 0]\n",
    "        # print(f\"dbg: {b_obs[key].shape}, {b_obs[key].unsqueeze(1).shape}\")\n",
    "\n",
    "    # Extract only player 0's data\n",
    "    b_actions = rb_actions[:, 0]  # Keep dimension for compatibility\n",
    "    b_logprobs = rb_logprobs[:, 0]\n",
    "    b_values = rb_values[:, 0]\n",
    "    b_advantages = rb_advantages[:, 0]\n",
    "    b_returns = rb_returns[:, 0]\n",
    "\n",
    "    # 6. PPO update\n",
    "    clip_fracs = []\n",
    "    pg_losses = []\n",
    "    v_losses = []\n",
    "    \n",
    "    total_batch = num_steps\n",
    "    indices = np.arange(total_batch)\n",
    "    \n",
    "    for _ in range(train_epochs):\n",
    "        np.random.shuffle(indices)\n",
    "        for start in range(0, total_batch, batch_size):\n",
    "            end = start + batch_size\n",
    "            if end > total_batch:\n",
    "                continue  # Skip incomplete batches\n",
    "                \n",
    "            batch_inds = indices[start:end]\n",
    "\n",
    "            mb_obs = {k: v[batch_inds] for k, v in b_obs.items()}\n",
    "            mb_actions = b_actions[batch_inds]\n",
    "            mb_old_logprob = b_logprobs[batch_inds]\n",
    "            mb_adv = b_advantages[batch_inds]\n",
    "            mb_returns = b_returns[batch_inds]\n",
    "            mb_values = b_values[batch_inds]\n",
    "            \n",
    "            # Forward pass with current parameters\n",
    "            _, new_logprob, entropy, value = agent.get_action_and_value(mb_obs, action=mb_actions)\n",
    "            \n",
    "            # Important: since we're only updating player 0's policy\n",
    "            new_logprob = new_logprob.view(-1)\n",
    "            value = value.view(-1)\n",
    "            \n",
    "            logratio = new_logprob - mb_old_logprob\n",
    "            ratio = logratio.exp()\n",
    "            \n",
    "            # Normalize advantages\n",
    "            mb_adv = (mb_adv - mb_adv.mean()) / (mb_adv.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_adv * ratio\n",
    "            pg_loss2 = -mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "            pg_losses.append(pg_loss.item())\n",
    "\n",
    "            # Value loss\n",
    "            v_loss_unclipped = (value - mb_returns) ** 2\n",
    "            v_clipped = mb_values + torch.clamp(\n",
    "                value - mb_values, -clip_coef, clip_coef\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
    "            v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
    "            v_losses.append(v_loss.item())\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = pg_loss + vf_coef * v_loss - ent_coef * entropy_loss\n",
    "\n",
    "            # Gradient step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate clip fraction\n",
    "            clip_fraction = ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "            clip_fracs.append(clip_fraction)\n",
    "\n",
    "    # Return episode metrics\n",
    "    return {\n",
    "        \"episode_length\": end_step,\n",
    "        \"player0_return\": total_episodic_return[0].item(),\n",
    "        \"player1_return\": total_episodic_return[1].item(),\n",
    "        \"policy_loss\": np.mean(pg_losses),\n",
    "        \"value_loss\": np.mean(v_losses),\n",
    "        \"clip_fraction\": np.mean(clip_fracs),\n",
    "        \"win\": won\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_with_league(\n",
    "    LuxAICAECEnv(LuxAIS3GymEnv(numpy_output=True)),\n",
    "    total_episodes=1000,\n",
    "    rollout_num_steps=512,\n",
    "    snapshot_freq=50,\n",
    "    num_exploiters=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
